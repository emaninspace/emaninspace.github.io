[
["index.html", "Learn Statistics with Public Use Microdata Data Informed Decision Making for Profit and Power Welcome", " Learn Statistics with Public Use Microdata Data Informed Decision Making for Profit and Power Emmanuel A. Garcia Spring 2019, 2020-04-29 Welcome These are the course notes for MATH-159. "],
["preface.html", "Preface", " Preface If you have a burning question that you’d like to answer using data but have no idea how, this is the book for you. In the late 10’s/early 20’s, everyone is claiming to be “data informed”. Are you? Probably not. Are they? Probably also not. This book will take you through all the steps necessary to use data in whatever it is you do. This book has been a labor of love and grief. It was greatly influenced by one year of teaching AP Statistics at Saint Francis High School in Mountain View, CA and three years of teaching Introductory Statistics at Ohlone College in Fremont, CA. I was disheartened to find that statistics is taught at most high schools and community colleges as if the computer didn’t exist1. It was also shaped by experiences at Ohlone College trying to use data to convince fellow faculty and administrators to change practices and policies. This is the course that I wish my colleagues had taken before we needed to talk about data. A course that stresses data literacy and hands on practical skills over formula memorization. I didn’t love any of the “standard publisher” books. I was also unimpressed by most of the graphical statistics software for intro stats courses. These notes are heavily influenced by Robin Donatello’s course notes and ModernDive Statistics by Albert Kim. I am also grateful for the Census Beaurea’s excellent documentation and guides about ACS data. I’ve made some “opinionated” choices about coverage and software. Different choices could have been made. I think at this present time taken together as a whole they offer the smoothest path to using real world data. Practicality: Data in the real world is often messy and hard to use. You will work with real data that the U.S. Census Bureau releases about actual people from the very beginning. Researchers, pollsters, government workers, and many others use this data to make decisions that affect our lives everyday. You will gain the skills needed to work with data out in the real world. Philosophy: I think Philosophy of Science is completely ignored in the statistics curriculum. For instance, falsifiability is not difficult to explain and naturally leads to a null hypothesis. Software: This book takes a “tidyverse” approach (for the unfamiliar it’s a collection of modern packges for R). I hope this book first and foremost is useful for as many people as possible. The world is what it is. If a few extra people can use data to shine a light on what needs to change and make changes, the effort will have been worth it. Go forth and change the world. Emmanuel A. Garcia Sunnyvale, CA February 14, 2020 Lasted edited: 2020-04-29 14:25:10 PDT. A great irony when students aren’t paying attention because of the phone in their pockets.↩︎ "],
["preliminaries.html", "1 Preliminaries 1.1 Stages of the statistical process 1.2 Public Use Microdata 1.3 Using R", " 1 Preliminaries …all models are approximations. Essentially, all models are wrong, but some are useful. However, the approximate nature of the model must always be borne in mind…. — George Box (Statistician) The last hundred years, statistics has been an indispensable part of the scientific method–an attempt make statements (that are probably true) when one has imperfect information. It can roughly be broken down into three stages: Pose a question. Collect relevant data/information. Analyze the data and reach a conclusion. This is a simplified view and we will provide more nuance as the course continues. But it gets the point across, statistics is the study of posing (certain types of) questions, collecting data, and finally analyzing the data and reaching a conclusion. The process often needs to be repeated–answering one questions just raises another–in other words it is iterative. It is important to realize that statistics works in conjunction with another discipline. A biologist might use statistical methods to pose questions, gather information, and make conclusions about biological problems and so too does the physicist, sociologist and a host of others. Statistics is inherently cross disciplinary.2 It is much easier to learn this material if you have a question you need to answer with data. Many statistical methods were pioneered by scientists, not mathematicians, in pursuit of their scientific endeavours. World War II was a particularly fruitful time for practical statistics. Some war stories are tales of derring-do; others are more cerebral. Both types specialise in plot twists: wars knock people off one path and onto another. Lord Moser was one of dozens of young men in Britain who would never have become statisticians had it not been for the war, who helped win it for Britain and its allies—and who went on to transform statistics. Wars are when governments find the money and will to make things happen, says Eileen Magnello, a historian of statistics. As it mobilised and supplied troops, grew and rationed food, and retooled factories to make shells, planes and tanks, Britain’s government realised that its knowledge of the country’s society, economy and manufacturing skills was not up to the task—and that statisticians could help. https://www.economist.com/christmas-specials/2014/12/17/they-also-served The English needed to make the most of sparse farmland. They used statistics to find the most efficient methods of growing and distributing food. The quality of the average Englishman’s diet actually increased during the war. The same goes for production of all sorts of products. Quality and quantity needed to increase. How did they do it? They employed statistical analysis of the methods of production. As the war progressed and the value of statistics became clearer, some Cambridge mathematics students were drafted to statistical work. David Cox, a grammar-school boy from Birmingham, was among them. In 1944 he went to the Royal Aircraft Establishment, a research institute in Farnborough. “The naive assumption was that if you were good enough to get a good degree in mathematics you would pick up statistics in a week or two,” he says. He was part of a group that analysed the distribution of German bombs falling on London each day. They concluded that the Germans were trying to destroy the docks but missing. Sir David also worked on quality-control in the manufacture of aircraft components, and the calculation of the distribution of stresses on aircraft in flight. “The aim was to load planes up to the point that the wings were about to drop off,” he says. The research meant the RAF dropped more bombs, and brought more pilots safely home, than it would have otherwise. The war was responsible for creating a generation of data informed decision makers. It is hard to quantify how large of an effect this group had on the English economy for years afterwards but it certainly couldn’t have hurt the post-war boom. Peace finally returned, and the statistical scene in the United Kingdom had been completely transformed,” wrote Barnard and Plackett. “No other method would have produced these changes in only six years.” Dozens of clever young people had been taught a fast-changing new subject—and in many cases done original research. Even routine work was elevated by the urgency and camaraderie of the war effort—and even the fact that they were new to the field. “A lot of the work was statistically boring,” Sir David says now. “But the point is that I didn’t really know anything.” “After the war the section exploded like a London bomb into missionary statistical occupations all over the country,” wrote Geoffrey Jowett, one of the SR 17 alumni, in 1990. “In convincing others that we had a good product to sell we convinced ourselves.” He went on to do research in quality control at United Steel Companies in Sheffield, and then to Sheffield University, and thence to universities in Australia and New Zealand. (Same article) 1.1 Stages of the statistical process 1.1.1 Formulating Research Questions Research questions aren’t really questions which is sort of confusing. One often starts with a question such as “Why aren’t students passing my class?” but properly stated a research question is a statement. For example the question might turn into the statement “Students aren’t passing because the lecture method used in class is ineffective at conveying information” which hinges the statement “the lecture method is ineffective at conveying information”. Not all questions (statements?) are made equal no matter what your previous teachers might have told you. We want to deal with questions that meet at least these two criteria: They are falsifiable. They deal with more than two variables. 1.1.1.1 Falsifiability A falsifiable statement is one which can be shown wrong with evidence. Informally, a statement is falsifiable if some observation might show it to be false. For example, “All swans are white” is falsifiable because the observation “Here is a black swan” shows it to be false. https://en.wikipedia.org/wiki/Falsifiability Is my research question falsifiable? Yes, because one could setup an experiment3 to provide evidence that the lecture method is effective at conveying information thereby showing the statement wrong. It has been falsified. A non-falsifiable statement is “my boyfriend cheated on me”. It’s practically impossible to show the statement is false. Imagine that the statement is wrong—what evidence would there be? None because there is no evidence of not cheating unless one has extensive evidence (like 24 hour video footage covering the whole relationship). We can however turn it into a falsifiable statement with “my boyfriend didn’t cheat on me”. In most cases a falsifiable statement just needs one observation to disprove it. A Statement that is not falsifiable usually needs some sort of exhaustive search of all possibilities to disprove it https://courses.vcu.edu/PHY-rhg/astron/html/mod/006/index.html 1.1.1.2 Several Variables Interesting statements have a minimum amount of complexity. We are interested in finding associations between a pair of variables4. Variables are associated if there is a pattern in the occurrences of values5 of their respective variables. For instance, you may notice that happy teachers usually have students with high grades whereas unhappy teachers have students with low grades. The “happiness” of the teachers is associated with the “grades” of the student. Analyzing how happy teachers might be or the grades of students (separately) is interesting but it doesn’t tell us much. If we consider the two variables at the same time, we might be able to use our information to raise student grades. 1.1.2 Collecting Data One of the greatest discoveries in Statistics is that how the data is collected has a great influence on how useful it is. The ideal would be to have data on all the people you want to study. This is called a census which is a collection of data about an entire population (or all relevant people). It’s usually impractical to do a census so we settle for a sample (information on some of the population). 1.1.2.1 Random Sampling How a sample is chosen has far ranging effects. Choices about who or what to include in the sample bias the results. Consider how well a poll of San Francisco and Los Angeles will reflect the voting preference of the whole United States. The choice to only poll SF and LA biases the result. To avoid bias, the standard is to randomly choose the sample, in other words to “not” make a choice. How one chooses a sample is part of the larger study of sample design. Many investigations fail due to lack of planning i.e. poor sample design. It is a good idea to consult a statistician or plan carefully before you start. Statistics isn’t magic. Garbage planning will get you garbage results. To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. — Ronald Fisher (Biologist/Statistician) An interesting example of the effect of biased sampling is Berkson’s Paradox. This anecdote about why handsome men are always jerks does a good job of explaining. Suppose Alex will only date a man if his niceness plus his handsomeness exceeds some threshold. Then nicer men do not have to be as handsome to qualify for Alex’s dating pool. So, among the men that Alex dates, Alex may observe that the nicer ones are less handsome on average (and vice versa), even if these traits are uncorrelated in the general population. … The average nice man that Alex dates is actually more handsome than the average man in the population (since even among nice men, the ugliest portion of the population is skipped). Berkson’s negative correlation is an effect that arises within the dating pool: the rude men that Alex dates must have been even more handsome to qualify. https://en.wikipedia.org/wiki/Berkson%27s_paradox The perceived relationship between being a jerk and being handsome doesn’t hold true in general. It is an effect of how Alex chooses men i.e. how she samples from the population. 1.1.2.2 Drawing Conclusions Researchers are usually interested in whether two or more variables are associated. Ideally we would like to know if one variable effects the outcome another variable. What is the difference between association and effect? Association is a “soft statement” that values of one variable usually occur with values of another variable. Effect is a “strong statement”. Statistical research is generally divided into two cases: Observational Studies Experiments Observational studies imply that variables are merely associated. After pouring over the data you notice that certain values of a variable usually coincide with certain values of another variable. For instance, you notice that students who study a lot usually have high grades. Association doesn’t imply that more studying can increase higher grades. It simply states what was seen in the data. Experiments imply that one variable effected (caused) the outcome of another variable. In this case, we actually think more studying will lead to an increase in grades. 1.1.2.3 Random Assignment What makes the difference? In an experiment the researcher randomly assigns subjects to either a treatment or control group. Random choices reduce bias in the object of study which is why there is random sampling, random assignment, etc. It increases the chance that pattern you see is a reflection of reality and not noise. The statistical meaning of noise is a pattern that only occurs in your particular experiment by chance and will not be seen again. It’s like “listening” to the scratches and hisses i.e. noise on an old vinyl and mistaking it for the music. In the studying example that would mean that the researcher randomly assigns half of students to study 8 or more hours a week and the other half to study less than four hours a week or not at all6. If the analysis of the grade data shows that the students who studied more have higher grades then we can conclude that the studying caused the higher grades. In an observational study, who studies a lot or a little is determined by some other mechanism such as the students deciding to study or not. No amount of post facto analysis can change a study to an experiment as R.A. Fisher pointed out. If subjects aren’t randomly assigned to different groups then the only conclusion one can make is that the variables are associated. 1.1.3 Analysis We will use the programming language called R to do statistical analysis for us. R descended from the S language (which probably stood for Statistics). It’s used in industry and academia to do statistical research7. You will recognize R code because it will show up like below. r_command() This “step” is comprised of many smaller steps and maintain our focus for the rest of the semester. There is no general formulaic approach that always work. One often has to “play it by ear” so the goal is to teach you the most useful and common techniques. In general the, the analysis step requires at least: Loading the data into the computer. “Cleaning” and “transforming” the data into a form which is understandable and analyzable. Choosing an appropriate analysis method. Interpreting the results from the analysis in the context of the original question. 1.2 Public Use Microdata Microdata means that the data is about individuals and not groups. Public use means that it is distributed by whomever for the public at large to use. We will use microdata from the Census Bureau that is released every year called the American Community Survey (ACS). There is a wealth of public microdata available from many time periods. A rather recent development is the digitization of previous census information. Researchers have a plethora of microdata available sometimes stretching back to 1703 which allows researchers to analyze how populations have changed over time in a way not possible before.8 1.2.1 ACS The Census Bureau created the ACS because federal, state, and local government agencies and researchers needed information which is updated more often than the census which is only done every 10 years. The reality is that communities are changing so quickly that the once every 10 year snapshot of the census wasn’t cutting it for tracking trends and changes. The ACS is now often the primary source of information whereas the “census” is only legally needed to apportion representatives to the House. The ACS is a random sample of 3.5 million American households (approximately 1%). The information is used to determine how over 675 billion dollars are spent.9 It is a rolling sample–the information is collected over the course of a year. This is different from the census which is an attempt to collect information about every man, woman and child during April of the year it is performed. The Census Bureau spent a decade working on the method used to collect the information for the ACS regularly checking it against census information to assess it’s accuracy. The curious can an authoritative report on the design, implementation, and validation by the Census Bureau at https://www.census.gov/programs-surveys/acs/methodology/design-and-methodology.html. 1.3 Using R There are many ways to use R. I think the easiest way to start with with RMarkdown documents. I actually used them to write this book. An Rmarkdown document lets us mix code and words resulting in what is called literate programming. The literate programming style isn’t just code. It’s code meant be read like a novel. We mix words which explain what we are doing and why with code that “crunches the numbers” so that we can make a decision. This mixture forces us to remember that someone will be reading our work! Eyes usually glaze over when pie charts and tables of numbers show up during a power point presentation. Our goal is present an engaging and thoughtful narrative to guide the reader through the data. The famous statistics John Tukey said it best: “The best thing about being a statistician is that you get to play in everyone’s backyard.”↩︎ We will explain what exactly constitutes an experiment later so don’t worry if you don’t understand how an experiment works.↩︎ You can think of a variable as a question such as “How happy are you?”. A researcher will ask this question to many people and collect their responses to analyze.↩︎ A value of a variable is the answer a person gives to the researcher. Remember that the variable is the question. The response, “no”, is the value of the variable, “Are you happy?”. There will be as many values for a variable as there were people asked the question.↩︎ These groups are the treatment and control groups. The treatment is the group that did the action which you believe might cause higher grades–in this case studying more.↩︎ Facebook, Instagram, Twitter, Amazon, etc. use it to figure out what should show up in your feed, what to recommend, what is trending, etc.↩︎ http://users.hist.umn.edu/~ruggles/Articles/Big_Microdata.pdf↩︎ https://www.census.gov/content/dam/Census/programs-surveys/acs/guidance/training-presentations/20180214_PUMS.pdf↩︎ "],
["data-and-data-management.html", "2 Data and data management 2.1 First step–load packages 2.2 Second step–load data 2.3 Cleaning data 2.4 Saving your clean data to a file 2.5 Filtering rows 2.6 Counting and Summarizing", " 2 Data and data management This chapter covers how to get your hands on actual public microdata from the Census. I’ll be using 2018 ACS data for Alameda and Santa Clara county10 residents for almost all examples in this book. 2.1 First step–load packages The R programming is wonderful but it’s not perfect. There are addon packages/libraries. Games sometimes call them extension packs. There are two packages we always need to load dplyr and janitor. We will learn about more later but we only need these two for now. Load them with the command below. To load the libraries, copy the command below into an RMarkdown file. You will always start an analysis with loading libraries. library(dplyr) library(janitor) 2.2 Second step–load data The data is loaded using the load command. load(&quot;~/Data/ACS2018_ASC_raw.RData&quot;) Yay, the data is loaded. Now you need to find out what exactly we loaded. You can find the data available to you with the ls command. ls() ## [1] &quot;ACS2018_ASC_raw&quot; Apparently the data is saved with the name ACS2018_ASC_raw. 2.2.1 Viewing data After you’ve loaded the data. You probably want to see what the data contains. The easiest way is to just type the name of the data you want to see. ACS2018_ASC_raw ## # A tibble: 35,638 x 512 ## SERIALNO DIVISION SPORDER PUMA REGION ST ADJINC PWGTP AGEP CIT CITWP ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2018GQ0… 9 01 00101 4 06 1.01e6 48 19 5 NA ## 2 2018GQ0… 9 01 00106 4 06 1.01e6 52 21 1 NA ## 3 2018GQ0… 9 01 08507 4 06 1.01e6 33 65 1 NA ## 4 2018GQ0… 9 01 00103 4 06 1.01e6 20 23 1 NA ## 5 2018GQ0… 9 01 00106 4 06 1.01e6 9 48 1 NA ## 6 2018GQ0… 9 01 08512 4 06 1.01e6 14 89 4 1995 ## 7 2018GQ0… 9 01 08509 4 06 1.01e6 79 18 1 NA ## 8 2018GQ0… 9 01 08509 4 06 1.01e6 66 22 1 NA ## 9 2018GQ0… 9 01 00101 4 06 1.01e6 59 94 1 NA ## 10 2018GQ0… 9 01 08504 4 06 1.01e6 83 36 1 NA ## # … with 35,628 more rows, and 501 more variables: COW &lt;dbl&gt;, DDRS &lt;dbl&gt;, ## # DEAR &lt;dbl&gt;, DEYE &lt;dbl&gt;, DOUT &lt;dbl&gt;, DPHY &lt;dbl&gt;, DRAT &lt;dbl&gt;, DRATX &lt;dbl&gt;, ## # DREM &lt;dbl&gt;, ENG &lt;dbl&gt;, FER &lt;dbl&gt;, GCL &lt;dbl&gt;, GCM &lt;chr&gt;, GCR &lt;chr&gt;, ## # HINS1 &lt;dbl&gt;, HINS2 &lt;dbl&gt;, HINS3 &lt;dbl&gt;, HINS4 &lt;dbl&gt;, HINS5 &lt;dbl&gt;, ## # HINS6 &lt;dbl&gt;, HINS7 &lt;dbl&gt;, INTP &lt;dbl&gt;, JWMNP &lt;dbl&gt;, JWRIP &lt;dbl&gt;, JWTR &lt;chr&gt;, ## # LANX &lt;dbl&gt;, MAR &lt;dbl&gt;, MARHD &lt;dbl&gt;, MARHM &lt;dbl&gt;, MARHT &lt;dbl&gt;, MARHW &lt;dbl&gt;, ## # MARHYP &lt;dbl&gt;, MIG &lt;dbl&gt;, MIL &lt;dbl&gt;, MLPA &lt;dbl&gt;, MLPB &lt;dbl&gt;, MLPCD &lt;dbl&gt;, ## # MLPE &lt;dbl&gt;, MLPFG &lt;dbl&gt;, MLPH &lt;dbl&gt;, MLPI &lt;dbl&gt;, MLPJ &lt;dbl&gt;, MLPK &lt;dbl&gt;, ## # NWAB &lt;dbl&gt;, NWAV &lt;dbl&gt;, NWLA &lt;dbl&gt;, NWLK &lt;dbl&gt;, NWRE &lt;dbl&gt;, OIP &lt;dbl&gt;, ## # PAP &lt;dbl&gt;, RELP &lt;dbl&gt;, RETP &lt;dbl&gt;, SCH &lt;dbl&gt;, SCHG &lt;chr&gt;, SCHL &lt;chr&gt;, ## # SEMP &lt;dbl&gt;, SEX &lt;dbl&gt;, SSIP &lt;dbl&gt;, SSP &lt;dbl&gt;, WAGP &lt;dbl&gt;, WKHP &lt;dbl&gt;, ## # WKL &lt;dbl&gt;, WKW &lt;dbl&gt;, WRK &lt;dbl&gt;, YOEP &lt;dbl&gt;, ANC &lt;dbl&gt;, ANC1P &lt;chr&gt;, ## # ANC2P &lt;chr&gt;, DECADE &lt;dbl&gt;, DIS &lt;dbl&gt;, DRIVESP &lt;dbl&gt;, ESP &lt;chr&gt;, ESR &lt;dbl&gt;, ## # FOD1P &lt;dbl&gt;, FOD2P &lt;dbl&gt;, HICOV &lt;dbl&gt;, HISP &lt;chr&gt;, INDP &lt;chr&gt;, JWAP &lt;chr&gt;, ## # JWDP &lt;chr&gt;, LANP &lt;dbl&gt;, MIGPUMA &lt;chr&gt;, MIGSP &lt;chr&gt;, MSP &lt;dbl&gt;, ## # NAICSP &lt;chr&gt;, NATIVITY &lt;dbl&gt;, NOP &lt;chr&gt;, OC &lt;chr&gt;, OCCP &lt;chr&gt;, PAOC &lt;chr&gt;, ## # PERNP &lt;dbl&gt;, PINCP &lt;dbl&gt;, POBP &lt;chr&gt;, POVPIP &lt;dbl&gt;, POWPUMA &lt;chr&gt;, ## # POWSP &lt;chr&gt;, PRIVCOV &lt;dbl&gt;, PUBCOV &lt;dbl&gt;, QTRBIR &lt;dbl&gt;, RAC1P &lt;dbl&gt;, … There is too much data to fit so the computer luckily is smart enough to show only the first 10 rows and first several columns with a caveat that there are “35,628 more rows, and 501 more variables:”. The glimpse command gives an overview of the data. It lists how many rows the data has (35,638) and how many variables (512) as well as their names. It also shows different values from each variable. If you want to glimpse the data yourself run this command: glimpse(ACS2018_ASC_raw) 2.3 Cleaning data We are now ready to being the process of “cleaning” our data. Cleaning is the process of putting it into a form that is easiest to analyze and work with. For instance, you probably aren’t going to need all 512 variables so we will get rid of the ones we don’t need. There are also issues with how the data is “coded”. For instance, males are denoted with a 1 in the data instead of the word Male which is annoying. We need tools to make these changes. 2.3.1 The Pipe The pipe is one of our most powerful tools. It makes it easy to do multiple things to our data. If you want a command to do something to data you type data %&gt;% command. The pipe is the weird looking symbols, %&gt;%. data %&gt;% cut() %&gt;% marinate() %&gt;% cook() %&gt;% eat() tells R to load the data and then do several things to it. The first pipe tells R to take the data and cut it. The second pipe tells R to take the stuff which was cut and then marinate it. The third pipe tells R to take the cut and marinate(d) data and cook it. The fourth and final pipe tells R to take the cut, marinated(d) and cook(ed) data and eat it. You’ll use the pipe a lot to string together different commands. 2.3.2 Looking at one variable There’s no need to always see or work with all 512 variables. Instead we will select one or more variables needed for analysis. Let’s start with selecting a variable which tells us the subject’s sex. ACS2018_ASC_raw %&gt;% select(SEX) ## # A tibble: 35,638 x 1 ## SEX ## &lt;dbl&gt; ## 1 1 ## 2 1 ## 3 1 ## 4 2 ## 5 2 ## 6 2 ## 7 1 ## 8 2 ## 9 2 ## 10 1 ## # … with 35,628 more rows Success, we only see the sex of the subject. However it’s in a basically unreadable state. Why are there 1’s and 2’s? 2.3.3 Types of variables The Census Bureau saves all data as numbers for historical and other reasons. However not all variables that we will want to deal with are best recorded with numbers. Often we want a “character” or “categorical” response. I would answer “Male” to the question “What is your gender?” just like I would answer “5” to the question “How many sibblings do you have?”. Variables that have number responses are called numeric variables. Variables that have word responses are called categorical variables. According to the codebook: Character 1 Sex 1 .Male 2 .Female We need to recode the values by turning 1’s (numeric) into Male (categorical) and 2’s into Female. However we don’t want to change the SEX variable. Instead we will create a new variable called new_SEX. This is one of the tenets of reproducible research. Suppose someone sees your analysis using this data and then wants to do it themselves. They’ll be surprised when they download the ACS data and find out there is no Male but only 1 in the SEX variable if you don’t include the code that made the change. We will accomplish this task of recoding in two parts. 2.3.4 Mutate We create a new variable using mutate. mutate takes a variable and creates a new variable by doing something to the old variable. For instance, maybe we want to add 7 to everyone’s age. ACS2018_ASC_raw %&gt;% select(AGEP) %&gt;% mutate(new_AGEP = AGEP + 7) ## # A tibble: 35,638 x 2 ## AGEP new_AGEP ## &lt;dbl&gt; &lt;dbl&gt; ## 1 19 26 ## 2 21 28 ## 3 65 72 ## 4 23 30 ## 5 48 55 ## 6 89 96 ## 7 18 25 ## 8 22 29 ## 9 94 101 ## 10 36 43 ## # … with 35,628 more rows It’s fairly straight forward the equation in mutate is old variable = new variable (changed). 2.3.5 Recode The recode_factor or recode function does the heavy lifting of turning all 1’s into Male. The difference is whether you want to create a categorical/factor variable or not. recode(oldvarible, &quot;old value 1&quot; = &quot;new value 1&quot;, &quot;old value 2&quot; = &quot;new value 2&quot;) We put everything together to get this code which uses both mutate and recode to make a new SEX variable which says Male and Female. ACS2018_ASC_raw %&gt;% select(SEX) %&gt;% mutate(new_SEX = recode_factor(SEX, &quot;1&quot; = &quot;Male&quot;, &quot;2&quot; = &quot;Female&quot;)) ## # A tibble: 35,638 x 2 ## SEX new_SEX ## &lt;dbl&gt; &lt;fct&gt; ## 1 1 Male ## 2 1 Male ## 3 1 Male ## 4 2 Female ## 5 2 Female ## 6 2 Female ## 7 1 Male ## 8 2 Female ## 9 2 Female ## 10 1 Male ## # … with 35,628 more rows mutate creates a new variable called new_sex. How are we making the new variable? By recode(ing) the variable SEX. Notice how inside of the recode we explain how the old value of 1 is to be come Male and similarly for 2 and Female. It’s important to note that we did not change the SEX instead a new one was created. Also we didn’t add new_SEX to ACS2018_TriCity_raw (check yourself and see). You would need to use &lt;- to save the results with the new variable. Try and run the code above without the select(SEX) command. What is the difference? 2.3.6 Putting it all together with several variables Let’s say you only want these variables for your research questions: SEX, DEYE, DEAR, AGEP, WAGP, WKHP, NP, BDSP, RNTP, RAC1P, FINCP, HINCP, FS, HUPAC, PINCP, PUMA, and SERIALNO. There’s no point in keeping the other variables (they just clutter up the screen and waste memory) so let’s make a new raw data set with just those. mydata_raw &lt;- ACS2018_ASC_raw %&gt;% select(SEX, AGEP, DEYE, DEAR, RAC1P, # Personal characteristics WAGP, WKHP, PINCP, # Economic characteristics FINCP, HINCP, NP, NPF, BDSP, RNTP, MRGP, MV, FS, HUPAC, MSP, JWMNP, JWTR, PUMA, SERIALNO) It doesn’t print anything because R is saving the 23 variables into mydata_raw. Let’s glimpse it to see. mydata_raw %&gt;% glimpse ## Rows: 35,638 ## Columns: 23 ## $ SEX &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, … ## $ AGEP &lt;dbl&gt; 19, 21, 65, 23, 48, 89, 18, 22, 94, 36, 18, 64, 15, 18, 19, … ## $ DEYE &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ DEAR &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … ## $ RAC1P &lt;dbl&gt; 6, 1, 1, 1, 1, 1, 1, 1, 2, 8, 6, 1, 2, 1, 1, 9, 6, 2, 1, 1, … ## $ WAGP &lt;dbl&gt; 50, 7700, 5000, 6000, 0, 0, 500, 0, 0, 0, 1400, 0, 0, 0, 360… ## $ WKHP &lt;dbl&gt; 5, 20, 8, 50, NA, NA, 12, NA, NA, NA, 12, NA, NA, NA, 20, 40… ## $ PINCP &lt;dbl&gt; 50, 7700, 17200, 7500, 0, 15600, 500, 0, 4300, 0, 1400, 1740… ## $ FINCP &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ HINCP &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ NP &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ NPF &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ BDSP &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ RNTP &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ MRGP &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ MV &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ FS &lt;dbl&gt; 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, … ## $ HUPAC &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ MSP &lt;dbl&gt; 6, 6, 6, 6, 6, 3, 6, 6, 3, 6, 6, 4, 6, 6, 6, 6, 4, 6, 2, 6, … ## $ JWMNP &lt;dbl&gt; NA, 10, 25, 10, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ JWTR &lt;chr&gt; NA, &quot;10&quot;, &quot;01&quot;, &quot;10&quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ PUMA &lt;chr&gt; &quot;00101&quot;, &quot;00106&quot;, &quot;08507&quot;, &quot;00103&quot;, &quot;00106&quot;, &quot;08512&quot;, &quot;08509… ## $ SERIALNO &lt;chr&gt; &quot;2018GQ0000039&quot;, &quot;2018GQ0000045&quot;, &quot;2018GQ0000059&quot;, &quot;2018GQ00… Success. It only has the 23 variables for 35638 subjects. It’s simpler if you only work with the variables you need. Now we need to recode the variables and save them into a new dataframe called mydata_clean. The data dictionary says to make the following replacements for DEAR, DEYE and RAC1P: DEAR Hearing difficulty 1 .Yes 2 .No DEYE Vision difficulty 1 .Yes 2 .No RAC1P 1 .White alone 2 .Black or African American alone 3 .American Indian alone 4 .Alaska Native alone 5 .American Indian and Alaska Native tribes specified; or American Indian or Alaska Native, not specified and no other races 6 .Asian alone 7 .Native Hawaiian and Other Pacific Islander alone 8 .Some Other Race alone 9 .Two or More Races FS Yearly food stamp/Supplemental Nutrition Assistance Program (SNAP) recipiency b .N/A (vacant) 1 .Yes 2 .No HUPAC HH presence and age of children b .N/A (GQ/vacant) 1 .With children under 6 years only 2 .With children 6 to 17 years only 3 .With children under 6 years and 6 to 17 years 4 .No children JWTR Means of transportation to work bb .N/A (not a worker–not in the labor force, including persons .under 16 years; unemployed; employed, with a job but not at work; Armed Forces, with a job but not at work) 01 .Car, truck, or van 02 .Bus or trolley bus 03 .Streetcar or trolley car (carro publico in Puerto Rico) 04 .Subway or elevated 05 .Railroad 06 .Ferryboat 07 .Taxicab 08 .Motorcycle 09 .Bicycle 10 .Walked 11 .Worked at home 12 .Other method MSP Married, spouse present/spouse absent b .N/A (age less than 15 years) 1 .Now married, spouse present 2 .Now married, spouse absent 3 .Widowed 4 .Divorced 5 .Separated 6 .Never married MV When moved into this house or apartment b .N/A (GQ/vacant) 1 .12 months or less 2 .13 to 23 months 3 .2 to 4 years 4 .5 to 9 years 5 .10 to 19 years 6 .20 to 29 years 7 .30 years or more We won’t recode the other variables because they are actually numbers (discrete or continuous) and not categories. AGEP is the person’s age. WAGP is a person’s wage. WKHP is the number of hours the person worked the previous week. NP is the number of family members. BDSP is the number of bedrooms in the person’s house. RENTP is monthly rent. FINCP is total family income in the last 12 months. HINCP is the whole household’s income in the last 12 months. There might be multiple families in a household or what the census calls “sub-family” relationships. PINCP is total personal income in the last 12 months. SERIALNO is a unique number assigned to each household. Everyone with the same SERIALNO lives together which makes it possible to distinguish households and individuals. mydata_clean &lt;- mydata_raw %&gt;% mutate(new_SEX = recode_factor(SEX, &quot;1&quot; = &quot;Male&quot;, &quot;2&quot; = &quot;Female&quot;)) %&gt;% mutate(new_DEYE = recode_factor(DEYE, &quot;1&quot; = &quot;Eye difficulty&quot;, &quot;2&quot; = &quot;No eye difficulty&quot;)) %&gt;% mutate(new_DEAR = recode_factor(DEAR, &quot;1&quot; = &quot;Hearing difficulty&quot;, &quot;2&quot; = &quot;No hearing difficulty&quot;)) %&gt;% mutate(new_RAC1P = recode_factor(RAC1P, &quot;1&quot; = &quot;White alone&quot;, &quot;2&quot; = &quot;Black or African American alone&quot;, &quot;3&quot; = &quot;American Indian alone&quot;, &quot;4&quot; = &quot;Alaska Native alone&quot;, &quot;5&quot; = &quot;American Indian&quot;, &quot;6&quot; = &quot;Asian alone&quot;, &quot;7&quot; = &quot;Native Hawaiian and Other Pacific Islander alone&quot;, &quot;8&quot; = &quot;Some Other Race alone&quot;, &quot;9&quot; = &quot;Two or more races&quot;)) %&gt;% mutate(SERIALNO = factor(SERIALNO)) %&gt;% mutate(PUMA = factor(PUMA)) %&gt;% mutate(new_FS = recode_factor(FS, &quot;1&quot; = &quot;Food stamps&quot;, &quot;2&quot; = &quot;No food stamps&quot;)) %&gt;% mutate(new_HUPAC = recode_factor(HUPAC, &quot;1&quot; = &quot;With children under 6 years only&quot;, &quot;2&quot; = &quot;With children 6 to 17 years&quot;, &quot;3&quot; = &quot;With children under 6 years and 6 to 17 years&quot;, &quot;4&quot; = &quot;No children&quot;)) %&gt;% mutate(MV_new = recode_factor(MV, &quot;1&quot; = &quot;12 months or less&quot;, &quot;2&quot; = &quot;13 to 23 months&quot;, &quot;3&quot; = &quot;2 to 4 years&quot;, &quot;4&quot; = &quot;5 to 9 years&quot;, &quot;5&quot; = &quot;5 to 9 years&quot;, &quot;6&quot; = &quot;10 to 19 years&quot;, &quot;7&quot; = &quot;20 to 29 years&quot;, &quot;8&quot; = &quot;30 years or more&quot;, .ordered = TRUE)) %&gt;% mutate(JWTR_new = recode_factor(JWTR, &quot;01&quot; = &quot;Car, truck, or van&quot;, &quot;02&quot; = &quot;Bus or trolley bus&quot;, &quot;03&quot; = &quot;Streetcar&quot;, &quot;04&quot; = &quot;Subway&quot;, &quot;05&quot; = &quot;Railroad&quot;, &quot;06&quot; = &quot;Ferryboat&quot;, &quot;07&quot; = &quot;Taxicab&quot;, &quot;08&quot; = &quot;Motorcycle&quot;, &quot;09&quot; = &quot;Bicycle&quot;, &quot;10&quot; = &quot;Walked&quot;, &quot;11&quot; = &quot;Worked at home&quot;, &quot;12&quot; = &quot;Other method&quot;)) %&gt;% mutate(MSP_new = recode_factor(MSP, &quot;1&quot; = &quot;Now married, spouse present&quot;, &quot;2&quot; = &quot;Now married, spouse absent&quot;, &quot;3&quot; = &quot;Widowed&quot;, &quot;4&quot; = &quot;Divorced&quot;, &quot;5&quot; = &quot;Separated&quot;, &quot;6&quot; = &quot;Never married&quot;, .ordered = TRUE )) Let’s look at the changes. mydata_clean %&gt;% glimpse ## Rows: 35,638 ## Columns: 32 ## $ SEX &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2,… ## $ AGEP &lt;dbl&gt; 19, 21, 65, 23, 48, 89, 18, 22, 94, 36, 18, 64, 15, 18, 19,… ## $ DEYE &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,… ## $ DEAR &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,… ## $ RAC1P &lt;dbl&gt; 6, 1, 1, 1, 1, 1, 1, 1, 2, 8, 6, 1, 2, 1, 1, 9, 6, 2, 1, 1,… ## $ WAGP &lt;dbl&gt; 50, 7700, 5000, 6000, 0, 0, 500, 0, 0, 0, 1400, 0, 0, 0, 36… ## $ WKHP &lt;dbl&gt; 5, 20, 8, 50, NA, NA, 12, NA, NA, NA, 12, NA, NA, NA, 20, 4… ## $ PINCP &lt;dbl&gt; 50, 7700, 17200, 7500, 0, 15600, 500, 0, 4300, 0, 1400, 174… ## $ FINCP &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ HINCP &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ NP &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ NPF &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ BDSP &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ RNTP &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ MRGP &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ MV &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ FS &lt;dbl&gt; 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,… ## $ HUPAC &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ MSP &lt;dbl&gt; 6, 6, 6, 6, 6, 3, 6, 6, 3, 6, 6, 4, 6, 6, 6, 6, 4, 6, 2, 6,… ## $ JWMNP &lt;dbl&gt; NA, 10, 25, 10, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ JWTR &lt;chr&gt; NA, &quot;10&quot;, &quot;01&quot;, &quot;10&quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ PUMA &lt;fct&gt; 00101, 00106, 08507, 00103, 00106, 08512, 08509, 08509, 001… ## $ SERIALNO &lt;fct&gt; 2018GQ0000039, 2018GQ0000045, 2018GQ0000059, 2018GQ0000238,… ## $ new_SEX &lt;fct&gt; Male, Male, Male, Female, Female, Female, Male, Female, Fem… ## $ new_DEYE &lt;fct&gt; No eye difficulty, No eye difficulty, No eye difficulty, No… ## $ new_DEAR &lt;fct&gt; No hearing difficulty, No hearing difficulty, No hearing di… ## $ new_RAC1P &lt;fct&gt; Asian alone, White alone, White alone, White alone, White a… ## $ new_FS &lt;fct&gt; No food stamps, No food stamps, No food stamps, No food sta… ## $ new_HUPAC &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ MV_new &lt;ord&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ JWTR_new &lt;fct&gt; NA, &quot;Walked&quot;, &quot;Car, truck, or van&quot;, &quot;Walked&quot;, NA, NA, NA, N… ## $ MSP_new &lt;ord&gt; &quot;Never married&quot;, &quot;Never married&quot;, &quot;Never married&quot;, &quot;Never m… Never change the original data! If you recode a variable or change values, create a new one. This is one of the principles of reproducible analysis–don’t change the original data and document all changes. Now let’s remove the original variables. We will “subtract” them away using select. Remember that we need to save the changes with the arrow &lt;- mydata_clean &lt;- mydata_clean %&gt;% select(-SEX, -DEAR, -DEYE, -RAC1P, -FS, -HUPAC, -MV, -JWTR) Let’s glimpse the clean data. mydata_clean %&gt;% glimpse ## Rows: 35,638 ## Columns: 24 ## $ AGEP &lt;dbl&gt; 19, 21, 65, 23, 48, 89, 18, 22, 94, 36, 18, 64, 15, 18, 19,… ## $ WAGP &lt;dbl&gt; 50, 7700, 5000, 6000, 0, 0, 500, 0, 0, 0, 1400, 0, 0, 0, 36… ## $ WKHP &lt;dbl&gt; 5, 20, 8, 50, NA, NA, 12, NA, NA, NA, 12, NA, NA, NA, 20, 4… ## $ PINCP &lt;dbl&gt; 50, 7700, 17200, 7500, 0, 15600, 500, 0, 4300, 0, 1400, 174… ## $ FINCP &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ HINCP &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ NP &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ NPF &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ BDSP &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ RNTP &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ MRGP &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ MSP &lt;dbl&gt; 6, 6, 6, 6, 6, 3, 6, 6, 3, 6, 6, 4, 6, 6, 6, 6, 4, 6, 2, 6,… ## $ JWMNP &lt;dbl&gt; NA, 10, 25, 10, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ PUMA &lt;fct&gt; 00101, 00106, 08507, 00103, 00106, 08512, 08509, 08509, 001… ## $ SERIALNO &lt;fct&gt; 2018GQ0000039, 2018GQ0000045, 2018GQ0000059, 2018GQ0000238,… ## $ new_SEX &lt;fct&gt; Male, Male, Male, Female, Female, Female, Male, Female, Fem… ## $ new_DEYE &lt;fct&gt; No eye difficulty, No eye difficulty, No eye difficulty, No… ## $ new_DEAR &lt;fct&gt; No hearing difficulty, No hearing difficulty, No hearing di… ## $ new_RAC1P &lt;fct&gt; Asian alone, White alone, White alone, White alone, White a… ## $ new_FS &lt;fct&gt; No food stamps, No food stamps, No food stamps, No food sta… ## $ new_HUPAC &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ MV_new &lt;ord&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ JWTR_new &lt;fct&gt; NA, &quot;Walked&quot;, &quot;Car, truck, or van&quot;, &quot;Walked&quot;, NA, NA, NA, N… ## $ MSP_new &lt;ord&gt; &quot;Never married&quot;, &quot;Never married&quot;, &quot;Never married&quot;, &quot;Never m… 2.4 Saving your clean data to a file Now that we’ve created a new data frame with only the variables that we need and they’re clean, we can save it. We will save our data, myclean_data, into the Data directory as myclean_data.RData. You can load this clean version with only the variables you want in the future. save(mydata_clean, file = &quot;~/Data/output/ACS_clean.RData&quot;) Next time you need the data to analyze, simply load it with load(\"~/Data/output/ACS_clean.RData\"). The data will be named mydata_clean or whatever name you used. 2.5 Filtering rows Sometimes instead of selecting columns we only want to select certain rows. This is called filtering. We can filter out everything except women who have no hearing difficulty. You can put multiple criteria by separating with commas. Notice the use of the double equals ==. mydata_clean %&gt;% filter(new_SEX == &quot;Female&quot;, new_DEAR == &quot;No hearing difficulty&quot;) ## # A tibble: 17,592 x 24 ## AGEP WAGP WKHP PINCP FINCP HINCP NP NPF BDSP RNTP MRGP MSP JWMNP ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 23 6000 50 7500 NA NA 1 NA NA NA NA 6 10 ## 2 48 0 NA 0 NA NA 1 NA NA NA NA 6 NA ## 3 89 0 NA 15600 NA NA 1 NA NA NA NA 3 NA ## 4 22 0 NA 0 NA NA 1 NA NA NA NA 6 NA ## 5 94 0 NA 4300 NA NA 1 NA NA NA NA 3 NA ## 6 18 1400 12 1400 NA NA 1 NA NA NA NA 6 NA ## 7 18 0 NA 0 NA NA 1 NA NA NA NA 6 NA ## 8 25 39000 40 39000 NA NA 1 NA NA NA NA 6 10 ## 9 84 0 NA 50300 NA NA 1 NA NA NA NA 4 NA ## 10 34 0 NA 8800 NA NA 1 NA NA NA NA 6 NA ## # … with 17,582 more rows, and 11 more variables: PUMA &lt;fct&gt;, SERIALNO &lt;fct&gt;, ## # new_SEX &lt;fct&gt;, new_DEYE &lt;fct&gt;, new_DEAR &lt;fct&gt;, new_RAC1P &lt;fct&gt;, ## # new_FS &lt;fct&gt;, new_HUPAC &lt;fct&gt;, MV_new &lt;ord&gt;, JWTR_new &lt;fct&gt;, MSP_new &lt;ord&gt; All the new_SEX values are Female and new_DEAR values are No hearing difficulty because that was what we filtered. There are different values for new_DEYE because it wasn’t part of the filter. I want to bring your attention to where is says A tibble: 17,592 x 11. This means there were 17,592 people from Alameda and Santa Clara counties who were females with no hearing difficult. We have 11 variables about those people. 2.6 Counting and Summarizing 2.6.1 Categorical You can use the count command to count the number of occurences of each value in a categorical variable. For instance, count(new_Sex) will count how many times Male and Female occur in the variable new_Sex. mydata_clean %&gt;% count(new_SEX) ## # A tibble: 2 x 2 ## new_SEX n ## &lt;fct&gt; &lt;int&gt; ## 1 Male 17587 ## 2 Female 18051 There are several hundred more females than males in our sample. You can put several several variables at a time if you separate the variables with commas. I’m going to filter and count. I start by filtering for children i.e. age less than 18. I will then have the computer count how many childdren live in a household of one person, two people, and so on. mydata_clean %&gt;% filter(AGEP &lt; 18) %&gt;% count(NP) ## # A tibble: 15 x 2 ## NP n ## &lt;dbl&gt; &lt;int&gt; ## 1 1 62 ## 2 2 176 ## 3 3 1363 ## 4 4 2923 ## 5 5 1468 ## 6 6 634 ## 7 7 227 ## 8 8 158 ## 9 9 100 ## 10 10 51 ## 11 11 36 ## 12 12 6 ## 13 13 4 ## 14 14 12 ## 15 15 8 Who are these minors who live alone (NP = 1)? I can filter only those children and have the computer count by gender and age. mydata_clean %&gt;% filter(AGEP &lt; 18, NP == 1) %&gt;% count(new_SEX, AGEP) ## # A tibble: 11 x 3 ## new_SEX AGEP n ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Male 7 1 ## 2 Male 8 1 ## 3 Male 12 1 ## 4 Male 13 1 ## 5 Male 14 1 ## 6 Male 15 14 ## 7 Male 16 2 ## 8 Male 17 18 ## 9 Female 14 6 ## 10 Female 15 4 ## 11 Female 17 13 They’re mostly males although there are some 17, 15, and 14 year old females who live alone. There’s a nice command called tabyl in the janitor package which counts occurrences of values and gives percentages automatically for you. Notice that tabyl gives the same answer as count. The difference is in the inclusion of the percentage of the total. Apparently females are 50.7% compared to 49.3% for males in our sample. mydata_clean %&gt;% tabyl(new_SEX) ## new_SEX n percent ## Male 17587 0.4934901 ## Female 18051 0.5065099 It can count the occurrences for each combination of the values for two variables such as new_DEAR and new_SEX. How many feamles with hearing difficulty are there? Apparently 459. mydata_clean %&gt;% tabyl(new_DEAR, new_SEX) ## new_DEAR Male Female ## Hearing difficulty 492 459 ## No hearing difficulty 17095 17592 This is helpful for when you need to make sure you recoded a variable correctly. 2.6.2 Numeric The equivalent for numeric variables is the summarize command which works with other commands to give us an idea about what’s in the data. Suppose you want to know the minimum and maximum ages in the sample. mydata_clean %&gt;% summarize(min_age = min(AGEP), avg_age = mean(AGEP), max_age = max(AGEP), n=n()) ## # A tibble: 1 x 4 ## min_age avg_age max_age n ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0 39.8 94 35638 The youngest person in the sample is zero. The oldest is 94. The average age is 39.8 years. This is looking at everybody in the sample. What if you want to compare groups–maybe men and women? mydata_clean %&gt;% group_by(new_SEX) %&gt;% summarize(min_age = min(AGEP), avg_age = mean(AGEP), max_age = max(AGEP), n=n()) ## # A tibble: 2 x 5 ## new_SEX min_age avg_age max_age n ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Male 0 38.9 94 17587 ## 2 Female 0 40.7 94 18051 Not surprising, there were men and women between ages 0 and 94 just like the whole sample. Apparently women are slightly older on average–40.7 versus 38.9. This is interesting because the oldest men and women seem to be 94 but women on average are older. You can go to this website https://tigerweb.geo.census.gov/tigerweb/ to narrow down a search to a smaller part of the county using a PUMA (Public Use Microdata Area).↩︎ "],
["describing-distributions-of-variables.html", "3 Describing distributions of variables 3.1 Categorical Variables 3.2 Numeric variables", " 3 Describing distributions of variables Statistics can give one a “bird’s eyes view” of the world. Or perhaps you’ve heard the phrase “seeing the forest from the trees”. We want the “larger” picture of what is going on in a variable. Somehow we want a bird’s eye view summary of what is happening with all the different values of a variable. For example, suppose we have a variable about how anxious people are on a scale of 1–5. It would be nice to how much of the sample is distributed across the different values i.e. how many 1’s, 2’s, and so on… Hopefully most people are a 1 or 2 and not 5. If we want to talk about the larger picture of a variable, we discuss the distribution of that variable. Knowing the distribution of a variable is important. We don’t want to rely on anecdotal evidence by focusing on a handful of occurrences. There are two new libraries we will need to load, ggplot for graphing and knitr for making beautiful tables. Don’t also forget to load your recoded data. library(dplyr) # For pipe and other data commands library(janitor) # For tabyl library(ggplot2) # For plotting using ggplot() function library(knitr) # For making tables using kable() library(stringr) # Needed for an example load(&quot;~/Data/output/ACS_clean.RData&quot;) knitr::opts_chunk$set(warning=FALSE, message=FALSE, fig.width=5, fig.align=&quot;center&quot;) # No warnings 3.1 Categorical Variables The distribution of a categorical variable is described by the count or percentage (of the total) for each of the different values. This information is presented with a table. You can make a table with count or tabyl. I prefer the later since it automatically does percentages for you. It’s common to also use the word frequency instead of count likewise relative frequency for percentage. mydata_clean %&gt;% tabyl(new_RAC1P) ## new_RAC1P n percent ## White alone 16031 0.4498288344 ## Black or African American alone 1739 0.0487962287 ## American Indian alone 157 0.0044054100 ## Alaska Native alone 4 0.0001122397 ## American Indian 71 0.0019922555 ## Asian alone 12456 0.3495145631 ## Native Hawaiian and Other Pacific Islander alone 200 0.0056119872 ## Some Other Race alone 2946 0.0826645715 ## Two or more races 2034 0.0570739099 If you live in the Bay Area, the results aren’t that surprising. The two largest categories are White and Asian alone respectively at 44% and 35% approximately. Mixed race people are at 5%. If you’re having difficulty reading the decimals as percentages use the adorn_pct_formatting command. Using the kable command also makes the table more visually appealing. I will use it for the remainder of the book. mydata_clean %&gt;% tabyl(new_RAC1P) %&gt;% adorn_pct_formatting(digits = 1) %&gt;% kable() new_RAC1P n percent White alone 16031 45.0% Black or African American alone 1739 4.9% American Indian alone 157 0.4% Alaska Native alone 4 0.0% American Indian 71 0.2% Asian alone 12456 35.0% Native Hawaiian and Other Pacific Islander alone 200 0.6% Some Other Race alone 2946 8.3% Two or more races 2034 5.7% digits = 1 tells R to round at the first decimal place. If you want more or less precision change the number. Zero digits means a whole number. If you want to sort for smallest to largest (ascending order) use the arrange command. You have to tell R the variable by which to arrange/sort. Use the n variable which counts the occurrences of each value. mydata_clean %&gt;% tabyl(new_RAC1P) %&gt;% adorn_pct_formatting(digits = 1) %&gt;% arrange(n) %&gt;% kable() new_RAC1P n percent Alaska Native alone 4 0.0% American Indian 71 0.2% American Indian alone 157 0.4% Native Hawaiian and Other Pacific Islander alone 200 0.6% Black or African American alone 1739 4.9% Two or more races 2034 5.7% Some Other Race alone 2946 8.3% Asian alone 12456 35.0% White alone 16031 45.0% 3.1.1 Graphs A graph makes it easy to compare the relative size of groups since you can see the difference. For example if one group is 30% and the other is 60%, you might not realize the larger is twice the size of the other until you think about it but it’s immediately apparent in a graph. mydata_clean %&gt;% ggplot(aes(new_RAC1P)) + geom_bar() + ggtitle(&quot;Distribution of race variable&quot;) + xlab(&quot;Race&quot;) Like always, the command starts with data and is then piped to another command. We will always use ggplot to graph something. Inside of the ggplot command is aes(), the aesthetic command, which tells ggplot what variable we want to graph, in this case new_RAC1P. There are then other commands which modify and specify what and how we want to graph the variable. ggplot uses + instead of the pipe to add more commands. geom_bar() tells ggplot that we want a bar graph of a categorical variable. ggtitle sets the title of the graph. xlab set the x-axis label. Unfortunately it’s impossible to read the labels in the graph. You can fix this by turning the graph on its side with coord_flip and using a special function to make sure the descriptions aren’t too long using the str_wrap, string wrap, function. The 20 inside of str_wrap tells it to break up the words every 20 characters so the descriptions aren’t too long. mydata_clean %&gt;% ggplot(aes(new_RAC1P)) + geom_bar() + ggtitle(&quot;Distribution of race variable&quot;) + coord_flip() + aes(stringr::str_wrap(new_RAC1P, 20)) + xlab(&quot;Race&quot;) 3.2 Numeric variables The distributions of numeric variables are more complicated to describe than categorical variables. There are four major characteristics of numeric distributions. The shape of the distriution. Whether the distribution has outliers. The center of the distribution. The spread of the distribution. 3.2.1 Shape Our primary tool for describing the shape of a numerical distribution is the histogram. The histogram bins a range of numeric values together and counts how many are in each bin. The histogram of the AGEP variable might be binned with a range of ten. All people between zero and ten (not including ten) would be counted. This bin would be called [0,10). All people between ten and twenth (not including twenty) would be counted. This bin would be called [10,20). And so on… Each bin would be as tall as the number of people in it. Here is the distribution of the variable BDSP, number of bedrooms in a house, with bins of width one. mydata_clean %&gt;% distinct(SERIALNO, BDSP) %&gt;% ggplot(aes(BDSP)) + geom_histogram(binwidth=1, boundary=0, closed = c(&quot;left&quot;), color = &quot;white&quot;) The bins are one wide which means; The first bin counts everything in [0,1) i.e. it includes zero but not one; The first bin counts everything in [1,2) i.e. it includes one but not two; And so on… We are interested in the tallest or most occurring bin. The graph shows that is the bin for two bedrooms with more than 10,000 occurrences. Let’s call this the “hightpoint” of the graph. The highpoint is approximately in the middle so this graph is symmetric. Let’s look at the histogram of HINCP which is total household income over the last twelve months. I’ve decided on a bin width of 25,000 so we will bin incomes by increments of 25,000 dollars. mydata_clean %&gt;% distinct(SERIALNO, HINCP) %&gt;% filter(!is.na(HINCP), HINCP &gt;= 0) %&gt;% ggplot(aes(x=HINCP)) + geom_histogram(aes(y=..density..), binwidth = 50000, boundary = 0, closed = c(&quot;left&quot;), color = &quot;white&quot; ) + geom_density(color=&quot;blue&quot;) + xlab(&quot;Household income&quot;) + ggtitle(&quot;Household income (HINCP)&quot;) + scale_x_continuous(labels = scales::comma) If you look carefuly or zoom in, the tallest bin is the third bin which is [50000, 75000) but the whole graph seems to stretch all the way out to past two million. The high point is no longer in the middle. This graph is right skewed because the distance from the high point to the right side of the graph is longer than the high point to the left side of the graph. There’s also a black line following the histogram. It is called a density curve. It smooths out the rectangular bins of the histogram. It also shows you how far the data goes when the bins are too small to see. The density curve ends a little past 2.5 million. 3.2.2 Outliers Outliers are values that deviate significantly from the “norm”. They are small clumps of values (usually no more than a handful) that are isolated and “far away” from the rest of the values. It is easy to identify outliers by having R tell us how many values occurred in each bin. Go back and add this code to where you generated your variables: mydata_clean %&gt;% distinct(SERIALNO, HINCP, .keepall = TRUE) %&gt;% filter(!is.na(HINCP), HINCP &gt;= 0) %&gt;% mutate(HINCP_binned = cut(HINCP, breaks=seq(from=0, to=2600000, by=50000), dig.lab=7, right = FALSE )) %&gt;% tabyl(HINCP_binned) %&gt;% adorn_pct_formatting(digits=3) %&gt;% kable() HINCP_binned n percent [0,50000) 2660 21.028% [50000,100000) 2719 21.494% [100000,150000) 2188 17.296% [150000,200000) 1667 13.178% [200000,250000) 1157 9.146% [250000,300000) 714 5.644% [300000,350000) 409 3.233% [350000,400000) 295 2.332% [400000,450000) 166 1.312% [450000,500000) 85 0.672% [500000,550000) 57 0.451% [550000,600000) 201 1.589% [600000,650000) 79 0.625% [650000,700000) 57 0.451% [700000,750000) 57 0.451% [750000,800000) 35 0.277% [800000,850000) 30 0.237% [850000,900000) 11 0.087% [900000,950000) 11 0.087% [950000,1000000) 9 0.071% [1000000,1050000) 4 0.032% [1050000,1100000) 2 0.016% [1100000,1150000) 16 0.126% [1150000,1200000) 7 0.055% [1200000,1250000) 2 0.016% [1250000,1300000) 3 0.024% [1300000,1350000) 2 0.016% [1350000,1400000) 0 0.000% [1400000,1450000) 0 0.000% [1450000,1500000) 2 0.016% [1500000,1550000) 1 0.008% [1550000,1600000) 1 0.008% [1600000,1650000) 0 0.000% [1650000,1700000) 0 0.000% [1700000,1750000) 0 0.000% [1750000,1800000) 0 0.000% [1800000,1850000) 1 0.008% [1850000,1900000) 1 0.008% [1900000,1950000) 0 0.000% [1950000,2000000) 0 0.000% [2000000,2050000) 0 0.000% [2050000,2100000) 0 0.000% [2100000,2150000) 0 0.000% [2150000,2200000) 0 0.000% [2200000,2250000) 0 0.000% [2250000,2300000) 0 0.000% [2300000,2350000) 0 0.000% [2350000,2400000) 0 0.000% [2400000,2450000) 0 0.000% [2450000,2500000) 0 0.000% [2500000,2550000) 0 0.000% [2550000,2600000) 1 0.008% There are 1,406 households in the (50000,75000] bin. Down at the bottom is 1 household in the (2575000,2600000]. Not too surprisingly very few people make around 2.6 million. This household is an outlier because they’re the only household in that bin but more importantly there are about 20 empty bins between it and the next closest household in the (1875000,1900000] bin. 3.2.3 Center and Spread Center and spread are numerical measures of the distribution. They are single numbers that attempt to describe a characteristic of the distribution. Center attempts to measure/describe a “typical” value in the distribution. Spread attempts to describe how far values “typically” deviate from the center. There are many ways to describe what is “typical” (most common, in the middle, what the cool kids do, etc.) so there are many measures of center and so too for spread. We will have two ways to measure each. We will choose which to use depending on the shape of the distribution. If the distribution is symmetric: Mean and Standard Deviation as center and spread. If the distribution is skewed: Median and IQR. 3.2.3.1 Mean and Standard Deviation The distribution of how much households pay for rent is fairly symmetric with a high point in the distribution between $1,500 and $2,000 according to the histogram. Note that the histogram is binned by $500 increments which I hope is reasonable. According to our rule, the “typical” value is determined by the mean. mydata_clean %&gt;% distinct(SERIALNO, RNTP) %&gt;% ggplot(aes(x=RNTP)) + geom_histogram(aes(y=..density..), binwidth = 500, boundary = 0, closed = c(&quot;left&quot;), color = &quot;white&quot; ) + geom_density() The mean is computed by adding all the values (in this case rents) and dividing by the number of values. It’s probably the “average” you learned in high school and grade school. The mean is called a “summary statistic”. It attempts to summarize a distribution. Use the summarize command to compute it. You need to include the na.rm=TRUE option to tell R to ignore anyone whose rent value is blank with NA for example people with a mortgage. mydata_clean %&gt;% distinct(SERIALNO, RNTP) %&gt;% summarize(mean = mean(RNTP, na.rm=TRUE)) %&gt;% kable() mean 2004.136 The “typical” person in the distribution pays $2,004 in rent which is literally pretty close to the center of the distribution hence the name. Standard deviation is the mean of the square of the distance between all the values and the mean. Put more plainly it measures how far “typically” values are from the mean. mydata_clean %&gt;% distinct(SERIALNO, RNTP) %&gt;% summarize(mean = mean(RNTP, na.rm=TRUE), sd = sd(RNTP, na.rm=TRUE)) %&gt;% kable() mean sd 2004.136 892.5093 The standard deviation is $893 which means that “typically” some one pays rent that is somewhere between $893 more or less than the “typical” rent of $2,004. It tells you how “spread” out the amount people pay for rent is, hence the name spread. If the standard deviation is small, it means most people pay around the same amount. For example if the the standard deviation was $100, typically rents are situated within two hundred dollars of the average. We should also include the minimum and maximum values to round out the summary. mydata_clean %&gt;% distinct(SERIALNO, RNTP) %&gt;% summarize(min = min(RNTP, na.rm=TRUE), mean = mean(RNTP, na.rm=TRUE), sd = sd(RNTP, na.rm=TRUE), max = max(RNTP, na.rm=TRUE)) %&gt;% kable() min mean sd max 4 2004.136 892.5093 3900 3.2.3.2 Median and IQR We know that household income is not symmetric. It is right skewed. The mean cannot be used as a measure of center for the “typical” household’s income instead we use median. The median is the “person in the middle”. If we were to line up everybody in Alameda and Santa Clara counties in order of lowest household income to largest household income, the income of the household in the middle of the line is the median of the distribution. This means that half of households are richer than the median and half are poorer. mydata_clean %&gt;% distinct(SERIALNO, HINCP) %&gt;% summarize_at(vars(HINCP), list(Min. = min, Mean = mean, Median = median, Max. = max, IQR = IQR), na.rm=TRUE) %&gt;% kable() Min. Mean Median Max. IQR -4800 160302.7 120000 2580000 147150 The “middle” or “typical” household makes $120,000 in Alameda and Santa Clara counties. Half make less and half make more. This number is different than the mean which is about $160,000. Why the difference? The high earning outliers we saw earlier “pull up” the average earnings. The mean is always closer to the outliers than it is to the middle of the distribution which is why we use it for skewed distributions. There five numbers to summarize the distribution. The minimum value of the distribution which in this case is -$4,800. Some household had negative earnings apparently. The 1st Quartile of the distribution. The 1st Quartile is the household income which 25% of households make less than. The *median of the distribution. This is the household income which 50% of households make less than. The 3rd Quartile of the distribution. This 3rd Quartile is the household income which 75% of households make less than. The maximum or the maximum of the “normal values”. Everything past the maximum or minimum is an outlier and shows up as a circle. The IQR measures how “spread out” the incomes of the middle 50% of households. They are spread out between 73575 above and below the median of 120,000. mydata_clean %&gt;% distinct(SERIALNO, HINCP) %&gt;% summarize_at(vars(HINCP), list(Min. = min, &quot;1st Qu.&quot; = ~quantile(., 0.25, na.rm=TRUE), Mean = mean, Median = median, &quot;3rd Qu.&quot; = ~quantile(., 0.75, na.rm=TRUE), Max. = max, IQR = IQR, &quot;n&quot; = ~length(.)), na.rm=TRUE) %&gt;% kable() Min. 1st Qu. Mean Median 3rd Qu. Max. IQR n -4800 58500 160302.7 120000 205650 2580000 147150 14112 mydata_clean %&gt;% summarize_at(vars(PINCP), list(Min. = min, &quot;1st Qu.&quot; = ~quantile(., 0.25, na.rm=TRUE), Mean = mean, Median = median, &quot;3rd Qu.&quot; = ~quantile(., 0.75, na.rm=TRUE), Max. = max, IQR = IQR, &quot;n&quot; = ~length(.)), na.rm=TRUE) %&gt;% kable() Min. 1st Qu. Mean Median 3rd Qu. Max. IQR n -4800 8800 68777.9 36900 90000 1325300 81200 35638 The results: The lowest household income was negative. 25% of households made less than 56 thousand approximately. Half of households make less than 120 thousand. 75% of households make less than 205 thousand. These numbers are displayed on a boxplot with vertical lines that form a box. The boxplot is also useful since it easy to see outliers. The dark line on the left is the median at $120,000. The circles off on the right are probable outliers past the “normal maximum” of about 500 thousand. mydata_clean %&gt;% # distinct(SERIALNO, HINCP) %&gt;% ggplot(aes(x=0, y=WAGP)) + geom_boxplot() + coord_flip() + scale_y_continuous(labels = scales::comma) + ggtitle(&quot;Total household income&quot;) "],
["relationships-between-variables.html", "4 Relationships between variables 4.1 Falsifiability 4.2 Two categorical variables 4.3 Numeric and Categorical Variable 4.4 Two Numeric Variables", " 4 Relationships between variables Now we are going to look at how the distribution of one variable changes (or fails to change) with another variable. The goal is to recognize association between variables. The analysis is different given the types of pairings of variables. Two categorical variables. Does membership in one group coincide with membership in another group? Is there a relationship between gender and owning a truck? Those are two categorical variables. The first is sex. The second is whether the person owns a truck. It might be that certain values of the sex variable, “male”, are more likely to coincide with certain values of the owns a truck variable, “yes”. A numeric variable and a categorical variable. In this case, usually one “groups” the sample by the categorical variable and analyzes the distribution of the numeric variable in each of the groups. For instance, we could analyze the distribution of the total household income variable “grouped” by race. In other words, analyze each of the races separately. Maybe some groups have a higher typical income than others and so on. Two numeric variables. This is different than the other two since we don’t “break up” the sample into groups. Instead we look at the two variables for each subject at the same time. The question is whether the the two variables increase/decrease together or there is no pattern. A good example is the variables shoe size and height. If you were to look at the data you would notice that in general people with small shoe sizes are usually short and that people with large shoe sizes are usually tall. In fact, you can probably estimate how tall someone is using their shoe size. Before we start, load the data and packages, and also set the options. knitr::opts_chunk$set(warning=FALSE, message=FALSE, fig.width=6, fig.align=&quot;center&quot;) # No warnings library(dplyr) # For pipe and other data commands library(janitor) # For tabyl library(ggplot2) # For plotting using ggplot() function library(knitr) # For making tablues using kabble() library(stringr) # Needed for an example library(kableExtra) # For styling library(ggmosaic) # For mosaic plot load(&quot;~/Data/output/ACS_clean.RData&quot;) 4.1 Falsifiability Hypotheses must be “falsifiable” e.g. able to be falsified. It needs to be possible to show that they’re are wrong. Because of this, the hypothesis usually is that the two variables are independent. If we see evidence that the variables are associated, we can reject hypothesis of independence. Assuming the variables are associated won’t work since it’s impossible to provide evidence of independence. 4.2 Two categorical variables We can look at two categorical variables at the same time with a two-way table. The table below shows there are 7,963 females who are also white alone in our sample. mydata_clean %&gt;% tabyl(new_RAC1P, new_SEX) %&gt;% kable() %&gt;% kable_styling(full_width = F, position=&quot;center&quot;) new_RAC1P Male Female White alone 8068 7963 Black or African American alone 839 900 American Indian alone 70 87 Alaska Native alone 0 4 American Indian 33 38 Asian alone 6034 6422 Native Hawaiian and Other Pacific Islander alone 99 101 Some Other Race alone 1443 1503 Two or more races 1001 1033 The adorn_totals command can “add” or “total” all the columns or rows. You specify which of the two, or both, inside of the command. For instance, there are a total of 71 American Indians of which 38 are female. We also added kable_styling which prints the table in a nicer format. mydata_clean %&gt;% tabyl(new_RAC1P, new_SEX) %&gt;% adorn_totals(c(&quot;row&quot;, &quot;col&quot;)) %&gt;% kable() %&gt;% kable_styling(full_width = F, position=&quot;center&quot;) new_RAC1P Male Female Total White alone 8068 7963 16031 Black or African American alone 839 900 1739 American Indian alone 70 87 157 Alaska Native alone 0 4 4 American Indian 33 38 71 Asian alone 6034 6422 12456 Native Hawaiian and Other Pacific Islander alone 99 101 200 Some Other Race alone 1443 1503 2946 Two or more races 1001 1033 2034 Total 17587 18051 35638 There are two types of percentages in a two-way table–row percentages and column percentages. As an example, there are 38 American Indian females. We can divide 38 either by 71 (the row total which is the total number of American Indians) or 18,051 (the column total which is the total number of Females). This example divides each number by the total of the “row” whence it comes. The 38 American Indian Females are 53.5% of American Indians. mydata_clean %&gt;% tabyl(new_RAC1P, new_SEX) %&gt;% adorn_totals(c(&quot;col&quot;)) %&gt;% adorn_percentages(&quot;row&quot;) %&gt;% adorn_pct_formatting(digits = 1) %&gt;% kable() %&gt;% kable_styling(full_width = F, position=&quot;center&quot;) new_RAC1P Male Female Total White alone 50.3% 49.7% 100.0% Black or African American alone 48.2% 51.8% 100.0% American Indian alone 44.6% 55.4% 100.0% Alaska Native alone 0.0% 100.0% 100.0% American Indian 46.5% 53.5% 100.0% Asian alone 48.4% 51.6% 100.0% Native Hawaiian and Other Pacific Islander alone 49.5% 50.5% 100.0% Some Other Race alone 49.0% 51.0% 100.0% Two or more races 49.2% 50.8% 100.0% This examples divides each number by the total of the “column” whence it comes. The 38 American Indian Females are 0.2% of Females. It also features the adorn_ns command which puts the number of the value in parentheses next to the percentage. mydata_clean %&gt;% tabyl(new_RAC1P, new_SEX) %&gt;% adorn_totals(c(&quot;row&quot;)) %&gt;% adorn_percentages(&quot;col&quot;) %&gt;% adorn_pct_formatting(digits = 1) %&gt;% adorn_ns() %&gt;% kable() %&gt;% kable_styling(full_width = F, position=&quot;center&quot;) new_RAC1P Male Female White alone 45.9% (8068) 44.1% (7963) Black or African American alone 4.8% (839) 5.0% (900) American Indian alone 0.4% (70) 0.5% (87) Alaska Native alone 0.0% (0) 0.0% (4) American Indian 0.2% (33) 0.2% (38) Asian alone 34.3% (6034) 35.6% (6422) Native Hawaiian and Other Pacific Islander alone 0.6% (99) 0.6% (101) Some Other Race alone 8.2% (1443) 8.3% (1503) Two or more races 5.7% (1001) 5.7% (1033) Total 100.0% (17587) 100.0% (18051) 4.2.1 Row and Column Percentages I want to draw attention to the two very similar but also very different statements we made. The 38 American Indian Females are 53.5% of American Indians. The 38 American Indian Females are 0.2% of Females. We are dealing with two variables–sex and race. There are 38 American Indian (a race) Females (a sex) in both statements. The difference is the ordering of the variables which specifies the order in which we are dividing our population by those variables. For example, we can divide the sample first by sex. All the males go to one side of the “room” and the females to the other. Any analysis performed thereafter would treat the two groups separately. Maybe we are curious about the proportion of “American Indians”. We know that “American Indians” account for 0.2% of “females”. We first split by gender so we say “of females” since our analysis is restricted to looking at females separately from men. Likewise we can divide the sample first by race. Each of the races will form a group. Any analysis will again treat the races separately. Perhaps this time we are curious about the proportion of “females”. We know that “females” account for 53.5% of “American Indians”. The variable which we first split by goes after “of”. We can first split by gender. I call each gender is a “large group”. We can then split the genders by “race”. I call each “race” (in a gender) a “small group”. The formula to follow is: The people who are _______ (smaller group) are X% of ________ (larger group). Both statements below still refer to the 38 “American Indian” “females” but the structure specifies the order of splitting used to get to the group. The people who are “female” are 53.5% of “American Indians”. The people who are “American Indian” are 0.2% of “females”. 4.2.2 Graphing – Bar Plot mydata_clean %&gt;% group_by(new_SEX, new_RAC1P) %&gt;% count() %&gt;% ggplot(aes(x = new_SEX, fill = new_RAC1P, y = n)) + geom_bar(position=&quot;stack&quot;, stat=&quot;identity&quot;) mydata_clean %&gt;% group_by(new_SEX, new_RAC1P) %&gt;% count() %&gt;% ggplot(aes(x = new_SEX, fill = new_RAC1P, y = n)) + geom_bar(position=&quot;fill&quot;, stat=&quot;identity&quot;) 4.2.3 An Example with a Hypothesis Let’s check if race and marriage status are associated. Our operating hypothesis is that race and marriage status are independent. This means that typically outcomes of the two combinations should be about the same. mydata_clean %&gt;% tabyl(new_RAC1P, MSP_new) %&gt;% adorn_totals(c(&quot;row&quot;)) %&gt;% adorn_percentages(&quot;row&quot;) %&gt;% adorn_pct_formatting(digits = 1) %&gt;% adorn_ns() %&gt;% kable() %&gt;% kable_styling(full_width = F, position=&quot;center&quot;) new_RAC1P Now married, spouse present Now married, spouse absent Widowed Divorced Separated Never married NA_ White alone 43.4% (6952) 2.3% (370) 4.5% (729) 7.8% (1257) 1.0% (161) 27.0% (4329) 13.9% (2233) Black or African American alone 22.3% (387) 4.1% (71) 6.4% (111) 12.2% (212) 2.3% (40) 41.2% (716) 11.6% (202) American Indian alone 36.9% (58) 3.8% (6) 7.0% (11) 11.5% (18) 3.8% (6) 24.2% (38) 12.7% (20) Alaska Native alone 25.0% (1) 0.0% (0) 0.0% (0) 25.0% (1) 0.0% (0) 50.0% (2) 0.0% (0) American Indian 35.2% (25) 8.5% (6) 2.8% (2) 2.8% (2) 0.0% (0) 26.8% (19) 23.9% (17) Asian alone 48.2% (6005) 4.3% (537) 3.3% (410) 3.4% (426) 0.7% (85) 23.8% (2969) 16.2% (2024) Native Hawaiian and Other Pacific Islander alone 42.0% (84) 4.0% (8) 4.0% (8) 5.5% (11) 1.0% (2) 29.0% (58) 14.5% (29) Some Other Race alone 30.8% (907) 3.6% (106) 2.7% (80) 5.4% (160) 1.5% (45) 34.6% (1018) 21.4% (630) Two or more races 21.6% (439) 1.7% (35) 1.4% (28) 5.2% (106) 0.7% (15) 30.8% (627) 38.5% (784) Total 41.7% (14858) 3.2% (1139) 3.9% (1379) 6.2% (2193) 1.0% (354) 27.4% (9776) 16.7% (5939) mydata_clean %&gt;% group_by(new_RAC1P, MSP_new) %&gt;% count() %&gt;% ggplot(aes(x = new_RAC1P, fill = MSP_new, y = n)) + geom_bar(position=&quot;fill&quot;, stat=&quot;identity&quot;) There does seem to be a strong association, particularly for the Never married value. African Americans especially are much more likely to have never been married. We can see from the data that people who are never married make up 41% of African Americans. For contrast, people who are never married typically make up around 30% of an ethnic category. If the variables were independent, the rate of Never married for African Americans would be closer to the other groups. Next chapter we will learn how to quantify how much variance/difference there can be between groups while stil remaining independent. 4.3 Numeric and Categorical Variable Often, we want to “break up” a sample into different groups to analyze separately. For instance, do all the races have a similar distribution of household income? The proper hypothesis would be that “race and household income are independent”. Use the group_by command to have R calculate separate numbers for each group. The order matters we are analyzing a numeric variable grouped/split by a categorical variable. mydata_clean %&gt;% distinct(SERIALNO, HINCP, .keep_all = TRUE) %&gt;% group_by(new_RAC1P) %&gt;% summarize_at(vars(HINCP), list(Min. = min, &quot;1st Qu.&quot; = ~quantile(., 0.25, na.rm=TRUE), Median = median, &quot;3rd Qu.&quot; = ~quantile(., 0.75, na.rm=TRUE), Max. = max, IQR = IQR, &quot;n&quot; = ~length(.)), na.rm=TRUE) %&gt;% kable() %&gt;% kable_styling(full_width = F, position=&quot;center&quot;) new_RAC1P Min. 1st Qu. Median 3rd Qu. Max. IQR n White alone -2400 60455 120600 210000.0 2580000 149545.0 7170 Black or African American alone 0 25075 60000 112025.0 1141200 86950.0 896 American Indian alone 0 36950 73000 142750.0 646100 105800.0 75 Alaska Native alone 100000 109500 119000 128500.0 138000 19000.0 2 American Indian 5400 45400 80000 105000.0 353000 59600.0 25 Asian alone -4800 70100 144000 231000.0 1886000 160900.0 4416 Native Hawaiian and Other Pacific Islander alone 0 48000 85920 134000.0 610000 86000.0 65 Some Other Race alone 0 45000 80000 125257.5 1194400 80257.5 905 Two or more races 0 65000 114000 199000.0 964000 134000.0 558 mydata_clean %&gt;% ggplot(aes(x=new_RAC1P, y=HINCP)) + geom_boxplot() + coord_flip() + scale_y_continuous(labels = scales::comma) + aes(stringr::str_wrap(new_RAC1P, 20)) + ylab(&quot;Race&quot;) Sometimes it’s useful to have the boxplot not show the outliers so that we only compare the “typical” people. It’s a bit more work because you also have to tell the computer where the graph should end. Inside of geom_boxplot put outlier.shape = NA so the computer doesn’t graph the outlier. Add an extra command ylim to specify the range of values. These should be the minimum value and another value larger than all the “typical” values. Usually this is the largest 3rd quartile seen from the table of summary statistics. mydata_clean %&gt;% ggplot(aes(x=new_RAC1P, y=HINCP)) + geom_boxplot(outlier.shape = NA) + aes(stringr::str_wrap(new_RAC1P, 20)) + coord_flip() + ylim(0, 450000) You can see that whites, asians and mixed race households have a higher typical range compared to other groups. However they aren’t that different in terms of “typical” values. The real difference is the size of the spreads. Whites, asians and mixed races households have a much larger spread compared to the other groups as seen by the size of the boxes. There doesn’t appear to be enough difference between the groups to legitimate rejecting the hypothesis of independence. The variables probably are independent. 4.3.1 Another Example Does the presence of children in a household effect the total household income? Maybe adults are motivated (or not) to make more money when children are present? We can look at how HINCP (household income) changes if we narrow our focus to households that do or no dot have children new_HUPAC. The properly formatted hypothesis is that “presence of children and household income are independent”. mydata_clean %&gt;% filter(!is.na(HINCP), !is.na(new_HUPAC)) %&gt;% distinct(SERIALNO, HINCP, new_HUPAC) %&gt;% group_by(new_HUPAC) %&gt;% summarize_at(vars(&quot;HINCP&quot;), list(Min. = min, &quot;1st Qu.&quot; = ~quantile(., 0.25, na.rm=TRUE), Median = median, &quot;3rd Qu.&quot; = ~quantile(., 0.75, na.rm=TRUE), Max. = max, IQR = IQR, &quot;n&quot; = ~length(.)), na.rm=TRUE) %&gt;% kable() %&gt;% kable_styling(full_width = F, position=&quot;center&quot;) new_HUPAC Min. 1st Qu. Median 3rd Qu. Max. IQR n With children under 6 years only 0 110000 171750 252575 1141200 142575 1006 With children 6 to 17 years -4800 85000 159000 260000 1886000 175000 2484 With children under 6 years and 6 to 17 years 0 72675 140000 230750 1160000 158075 686 No children -2400 48000 100000 180000 2580000 132000 8480 The medians are not too different so let’s check the boxplots. mydata_clean %&gt;% distinct(SERIALNO, HINCP, new_HUPAC) %&gt;% filter(!is.na(HINCP), !is.na(new_HUPAC)) %&gt;% ggplot(aes(x=new_HUPAC, y=HINCP)) + geom_boxplot() + aes(stringr::str_wrap(new_HUPAC, 20)) + coord_flip() + scale_y_continuous(labels = scales::comma) + xlab(&quot;Presence of children&quot;) + ggtitle(&quot;Effect of children on total income&quot;) Let’s remove the outliers. mydata_clean %&gt;% distinct(SERIALNO, HINCP, new_HUPAC) %&gt;% filter(!is.na(HINCP), !is.na(new_HUPAC)) %&gt;% ggplot(aes(x=new_HUPAC, y=HINCP)) + geom_boxplot(outlier.shape = NA) + aes(stringr::str_wrap(new_HUPAC, 20)) + scale_y_continuous(labels = scales::comma) + ylim(0, 500000) + coord_flip() There seems to be little difference between the groups. The “typical” ranges are not that different. Generally we don’t consider the distributions different unless the boxes don’t overlap at all i.e. the upper range in one group is smaller than the lower range for another group. We will learn how to use math to have the computer decide if the small difference is meaningful or not. There doesn’t appear to be enough difference between the groups to legitimate rejecting the hypothesis of independence. The variables probably are independent. 4.3.2 Another Let’s test the hypothesis that “length of tenancy in a dwelling and rent are indepdent”. mydata_clean %&gt;% filter(!is.na(RNTP), !is.na(MV_new)) %&gt;% distinct(SERIALNO, RNTP, MV_new) %&gt;% group_by(MV_new) %&gt;% summarize_at(vars(RNTP), list(Min. = min, &quot;1st Qu.&quot; = ~quantile(., 0.25, na.rm=TRUE), Median = median, &quot;3rd Qu.&quot; = ~quantile(., 0.75, na.rm=TRUE), Max. = max, IQR = IQR, &quot;n&quot; = ~length(.)), na.rm=TRUE) %&gt;% kable() %&gt;% kable_styling(full_width = F, position=&quot;center&quot;) MV_new Min. 1st Qu. Median 3rd Qu. Max. IQR n 12 months or less 4 1900 2400 3000 3900 1100 1221 13 to 23 months 160 1800 2300 2800 3900 1000 601 2 to 4 years 4 1500 2000 2600 3900 1100 1236 5 to 9 years 20 1100 1600 2200 3900 1100 1594 10 to 19 years 140 800 1200 1700 3900 900 169 20 to 29 years 100 745 1300 1850 3200 1105 67 mydata_clean %&gt;% filter(!is.na(RNTP), !is.na(MV_new)) %&gt;% distinct(SERIALNO, RNTP, MV_new) %&gt;% ggplot(aes(x=MV_new, y=RNTP)) + geom_boxplot() + coord_flip() + scale_y_continuous(labels = scales::comma) + xlab(&quot;Length of tenancy&quot;) + ggtitle(&quot;Effect of length of tenancy on rent&quot;) There does appear to be enough evidence to reject the independence hypothesis. The higher typical value of 20 to 29 years is not in the lower typical value of 12 months or less and there is a clear trending showing an increase in typical rent. The variables appear to be associated. 4.4 Two Numeric Variables Do people who make more money pay more for their rent/mortgage? Both are numerical variables. Perhaps total housing expenditure increases/decreases/unaffected by income. The hypothesis is “household income and rent are independent”. mydata_clean %&gt;% distinct(SERIALNO, HINCP, RNTP, .keep_all = TRUE) %&gt;% filter(HINCP &lt; 500000, !is.na(RNTP), !is.na(HINCP)) %&gt;% summarize(&quot;Average of HINCP&quot; = mean(HINCP, na.rm=TRUE), &quot;Average of RNTP&quot; = mean(RNTP, na.rm=TRUE), &quot;Correlation&quot; = cor(HINCP, RNTP, use=&quot;pairwise.complete.obs&quot;), &quot;n&quot; = length(SERIALNO) ) %&gt;% kable() %&gt;% kable_styling(full_width = F, position=&quot;center&quot;) Average of HINCP Average of RNTP Correlation n 102924.4 1989.874 0.5438235 4799 We first assess how strong the relationship between the two variables is with “correlation”. Correlation only checks to see if the two variables increase or decrease together. A correlation close to zero means there doesn’t seem to be a simple only increasing or decreasing relation. In this case there is a fairly strong relationship between how much a household earns and how much they spend on rent. The correlation of 0.54 means about 54% of how much rent changes can be explained by how much the household earns. Imagine comparing two households with different rents and total incomes. Suppose that you want to explain why household A pays x more dollars in rent than household B. A correlation of 0.54 means that you will be able to explain 54% of that difference by looking at the difference between their total incomes. How much we pay in rent doesn’t always come down to how much we make. Some people are willing to pay more or less because of other factors (how close it is to work, nice neighbors, better schools, etc.). The primary factor does seem to be how much we make. mydata_clean %&gt;% distinct(SERIALNO, HINCP, RNTP, .keep_all = TRUE) %&gt;% filter(HINCP &lt; 500000, !is.na(RNTP), !is.na(HINCP)) %&gt;% ggplot(aes(x=HINCP, y=RNTP)) + geom_jitter() + scale_x_continuous(labels = scales::comma) + scale_y_continuous(labels = scales::comma) + geom_smooth(se=FALSE, col=&quot;brown&quot;) + geom_smooth(se=FALSE, method=&quot;lm&quot;) There is a fairly strong linear relation. Rent increases as total household income also increases. There are very few high earners with low rents. There are also not that many low earners with high rent. This a scatter plot graph of the two variables. Each dot is a subject. The x value is the household’s total income. The y value is the household’s rent amount. The red line tracks how the variables are change together but is free to go up or down. The blue line also tracks how the variables change together but will only go up or down. It looks for a “linear relationship”. Correlation only measures linear relationships. Since the red line generally is only going up the linear relationship is fairly strong. 4.4.1 Homeowners The correlation of mortgage payment and total household income is not as strong as for rent. Only 38% of the difference in mortgage payments are explained by household income. mydata_clean %&gt;% distinct(SERIALNO, HINCP, MRGP, .keep_all = TRUE) %&gt;% filter(HINCP &lt; 500000, !is.na(MRGP), !is.na(HINCP)) %&gt;% summarize(&quot;Average of HINCP&quot; = mean(HINCP, na.rm=TRUE), &quot;Average of MRGP&quot; = mean(MRGP, na.rm=TRUE), &quot;Correlation&quot; = cor(HINCP, MRGP, use=&quot;pairwise.complete.obs&quot;), &quot;n&quot; = length(SERIALNO) ) %&gt;% kable() %&gt;% kable_styling(full_width = F, position=&quot;center&quot;) Average of HINCP Average of MRGP Correlation n 174396.3 2336.378 0.3809859 4882 You can see in this graph that many of the dots are clustered around the blue line. However there are some “outliers”–households which have unusually high mortgages given their household income. mydata_clean %&gt;% distinct(SERIALNO, HINCP, MRGP, .keep_all = TRUE) %&gt;% filter(HINCP &lt; 500000, !is.na(MRGP), !is.na(HINCP)) %&gt;% ggplot(aes(x=HINCP, y=MRGP)) + geom_jitter() + scale_x_continuous(labels = scales::comma) + scale_y_continuous(labels = scales::comma) + geom_smooth(se=FALSE, col=&quot;brown&quot;) + geom_smooth(se=FALSE, method=&quot;lm&quot;) Notice that on average homeowners have higher total household incomes–170 thousand versus 102 thousand. mydata_clean %&gt;% tabyl(new_RAC1P, PUMA) %&gt;% adorn_totals(c(&quot;row&quot;)) %&gt;% adorn_percentages(&quot;col&quot;) %&gt;% adorn_pct_formatting(digits = 1) %&gt;% adorn_ns() %&gt;% kable() %&gt;% kable_styling(full_width = F, position=&quot;center&quot;) new_RAC1P 00101 00102 00103 00104 00105 00106 00107 00108 00109 00110 08501 08502 08503 08504 08505 08506 08507 08508 08509 08510 08511 08512 08514 White alone 58.5% (1074) 42.1% (756) 62.7% (927) 22.5% (266) 44.8% (734) 46.0% (667) 31.7% (519) 24.9% (405) 28.2% (590) 57.9% (1389) 59.4% (1480) 38.7% (582) 48.9% (669) 18.6% (282) 29.9% (373) 75.9% (893) 49.5% (747) 48.1% (570) 50.0% (646) 50.7% (755) 61.7% (668) 55.4% (785) 20.6% (254) Black or African American alone 4.7% (87) 17.1% (306) 11.0% (162) 26.1% (309) 8.2% (134) 7.5% (109) 8.4% (138) 2.3% (38) 2.5% (53) 1.8% (44) 1.3% (32) 1.1% (16) 2.0% (28) 2.2% (33) 2.3% (29) 2.0% (23) 0.7% (11) 2.5% (30) 3.2% (41) 2.6% (38) 2.3% (25) 2.1% (30) 1.9% (23) American Indian alone 0.7% (13) 1.2% (21) 0.3% (5) 1.0% (12) 0.5% (8) 0.4% (6) 0.2% (3) 0.7% (11) 0.6% (13) 0.4% (10) 0.2% (4) 0.3% (5) 0.5% (7) 0.3% (4) 0.4% (5) 0.3% (4) 0.0% (0) 0.2% (2) 0.6% (8) 0.3% (5) 0.4% (4) 0.1% (1) 0.5% (6) Alaska Native alone 0.0% (0) 0.1% (1) 0.0% (0) 0.0% (0) 0.0% (0) 0.1% (1) 0.0% (0) 0.0% (0) 0.0% (1) 0.0% (1) 0.0% (0) 0.0% (0) 0.0% (0) 0.0% (0) 0.0% (0) 0.0% (0) 0.0% (0) 0.0% (0) 0.0% (0) 0.0% (0) 0.0% (0) 0.0% (0) 0.0% (0) American Indian 0.0% (0) 0.3% (5) 0.1% (2) 1.0% (12) 0.0% (0) 0.1% (1) 0.9% (14) 0.2% (4) 0.1% (2) 0.2% (4) 0.1% (2) 0.4% (6) 0.0% (0) 0.2% (3) 0.4% (5) 0.1% (1) 0.0% (0) 0.0% (0) 0.4% (5) 0.1% (2) 0.2% (2) 0.1% (1) 0.0% (0) Asian alone 24.0% (440) 24.0% (431) 14.1% (209) 8.5% (100) 30.8% (505) 27.1% (393) 30.2% (493) 52.7% (858) 56.8% (1188) 30.9% (741) 32.3% (805) 48.9% (735) 39.9% (546) 69.5% (1052) 45.3% (564) 12.1% (143) 45.4% (684) 41.1% (487) 20.4% (263) 31.0% (462) 27.0% (292) 32.7% (463) 48.7% (602) Native Hawaiian and Other Pacific Islander alone 0.6% (11) 0.8% (15) 0.3% (5) 1.4% (16) 1.0% (16) 1.4% (21) 2.4% (39) 1.0% (17) 1.0% (21) 0.6% (14) 0.1% (2) 0.1% (2) 0.2% (3) 0.0% (0) 0.2% (2) 0.1% (1) 0.0% (0) 0.0% (0) 0.4% (5) 0.3% (4) 0.1% (1) 0.1% (1) 0.3% (4) Some Other Race alone 3.9% (71) 8.4% (150) 3.1% (46) 33.0% (390) 7.5% (123) 11.0% (160) 18.8% (307) 11.1% (181) 5.6% (118) 2.8% (66) 2.2% (54) 4.1% (62) 4.0% (55) 4.4% (67) 17.9% (223) 5.4% (64) 0.4% (6) 4.1% (48) 18.3% (236) 9.5% (141) 2.9% (31) 3.7% (53) 23.8% (294) Two or more races 7.6% (139) 6.1% (109) 8.3% (122) 6.6% (78) 7.2% (118) 6.3% (92) 7.5% (122) 6.9% (113) 5.1% (107) 5.4% (129) 4.6% (114) 6.3% (94) 4.4% (60) 4.8% (73) 3.6% (45) 4.1% (48) 4.0% (60) 4.1% (48) 6.7% (87) 5.4% (81) 5.5% (60) 5.9% (83) 4.2% (52) Total 100.0% (1835) 100.0% (1794) 100.0% (1478) 100.0% (1183) 100.0% (1638) 100.0% (1450) 100.0% (1635) 100.0% (1627) 100.0% (2093) 100.0% (2398) 100.0% (2493) 100.0% (1502) 100.0% (1368) 100.0% (1514) 100.0% (1246) 100.0% (1177) 100.0% (1508) 100.0% (1185) 100.0% (1291) 100.0% (1488) 100.0% (1083) 100.0% (1417) 100.0% (1235) mydata_clean %&gt;% filter(!is.na(HINCP), !is.na(PUMA)) %&gt;% distinct(SERIALNO, HINCP, PUMA) %&gt;% group_by(PUMA) %&gt;% summarize_at(vars(HINCP), list(Min. = min, &quot;1st Qu.&quot; = ~quantile(., 0.25, na.rm=TRUE), Median = median, &quot;3rd Qu.&quot; = ~quantile(., 0.75, na.rm=TRUE), Max. = max, IQR = IQR, &quot;n&quot; = ~length(.)), na.rm=TRUE) %&gt;% kable() %&gt;% kable_styling(full_width = F, position=&quot;center&quot;) PUMA Min. 1st Qu. Median 3rd Qu. Max. IQR n 00101 0 40000.0 93435 182125.0 1325300 142125.0 684 00102 -2400 30000.0 73800 143600.0 958700 113600.0 829 00103 0 69000.0 140000 230150.0 1292000 161150.0 620 00104 0 24352.5 53800 93775.0 587200 69422.5 402 00105 -4800 50250.0 99400 160750.0 672000 110500.0 627 00106 0 50000.0 93950 159850.0 1141200 109850.0 486 00107 0 46807.5 85000 145047.5 704200 98240.0 512 00108 0 82955.0 126500 201000.0 840700 118045.0 515 00109 -2300 77750.0 146540 235015.0 1587450 157265.0 710 00110 -4600 83475.0 150000 228580.0 1130000 145105.0 840 08501 -2300 82625.0 177300 359450.0 2580000 276825.0 914 08502 0 78000.0 150000 254775.0 1008000 176775.0 578 08503 0 73300.0 130180 200000.0 744900 126700.0 513 08504 0 71030.0 135455 218470.0 831600 147440.0 466 08505 0 63900.0 122000 195700.0 782300 131800.0 357 08506 10 65000.0 117600 197000.0 990700 132000.0 389 08507 0 94000.0 188500 322400.0 1886000 228400.0 545 08508 0 61650.0 134000 210000.0 795000 148350.0 455 08509 -2300 47850.0 95000 175400.0 736300 127550.0 479 08510 0 46200.0 99400 191600.0 1137000 145400.0 541 08511 0 65900.0 131000 208225.0 835000 142325.0 380 08512 0 83025.0 157000 247505.0 1132400 164480.0 478 08514 0 40000.0 85000 143000.0 603140 103000.0 336 mydata_clean %&gt;% filter(!is.na(HINCP), !is.na(PUMA)) %&gt;% distinct(SERIALNO, HINCP, PUMA, new_RAC1P) %&gt;% group_by(PUMA, new_RAC1P) %&gt;% summarize_at(vars(HINCP), list(Min. = min, &quot;1st Qu.&quot; = ~quantile(., 0.25, na.rm=TRUE), Median = median, &quot;3rd Qu.&quot; = ~quantile(., 0.75, na.rm=TRUE), Max. = max, IQR = IQR, &quot;n&quot; = ~length(.)), na.rm=TRUE) %&gt;% kable() %&gt;% kable_styling(full_width = F, position=&quot;center&quot;) PUMA new_RAC1P Min. 1st Qu. Median 3rd Qu. Max. IQR n 00101 White alone 0 51700.0 114000 215000.0 1325300 163300.0 501 00101 Black or African American alone 0 20285.0 54000 145300.0 805000 125015.0 47 00101 American Indian alone 0 25000.0 40600 115900.0 162000 90900.0 9 00101 Asian alone 0 31500.0 75600 174000.0 805000 142500.0 189 00101 Native Hawaiian and Other Pacific Islander alone 8300 20750.0 49100 98800.0 175300 78050.0 4 00101 Some Other Race alone 2400 26800.0 76000 155000.0 805000 128200.0 27 00101 Two or more races 0 50450.0 130100 241125.0 805000 190675.0 88 00102 White alone -2400 55200.0 105000 180000.0 726000 124800.0 425 00102 Black or African American alone 0 19090.0 47700 104300.0 565000 85210.0 171 00102 American Indian alone 4000 21425.0 29500 66400.0 138000 44975.0 12 00102 Alaska Native alone 240500 240500.0 240500 240500.0 240500 0.0 1 00102 American Indian 5400 81000.0 134000 144700.0 250000 63700.0 5 00102 Asian alone 0 20900.0 70000 140000.0 460000 119100.0 221 00102 Native Hawaiian and Other Pacific Islander alone 10400 39475.0 56000 93750.0 172060 54275.0 8 00102 Some Other Race alone 1200 40000.0 91000 148150.0 958700 108150.0 60 00102 Two or more races 9000 75000.0 135000 197000.0 958700 122000.0 85 00103 White alone 0 80350.0 162800 256612.5 1292000 176262.5 446 00103 Black or African American alone 0 42250.0 100000 171600.0 1166700 129350.0 95 00103 American Indian alone 79000 82000.0 104000 240700.0 646100 158700.0 5 00103 American Indian 300000 300000.0 300000 300000.0 300000 0.0 1 00103 Asian alone 0 77700.0 140800 230600.0 1130000 152900.0 117 00103 Native Hawaiian and Other Pacific Islander alone 113000 115250.0 117500 119750.0 122000 4500.0 2 00103 Some Other Race alone 0 69300.0 112500 176250.0 435700 106950.0 28 00103 Two or more races 8000 91850.0 160000 230300.0 968000 138450.0 71 00104 White alone 0 35500.0 77600 135855.0 296000 100355.0 110 00104 Black or African American alone 0 20550.0 53265 84925.0 262200 64375.0 152 00104 American Indian alone 65000 86650.0 113560 115630.0 154000 28980.0 6 00104 American Indian 50800 54000.0 80000 97700.0 105000 43700.0 5 00104 Asian alone 0 24237.5 52550 106400.0 328100 82162.5 46 00104 Native Hawaiian and Other Pacific Islander alone 30000 37875.0 56250 75925.0 87700 38050.0 4 00104 Some Other Race alone 0 31800.0 50000 86900.0 587200 55100.0 107 00104 Two or more races 4500 39230.0 53250 86800.0 233000 47570.0 44 00105 White alone 0 58500.0 100000 169000.0 672000 110500.0 359 00105 Black or African American alone 0 31075.0 80300 127950.0 488000 96875.0 60 00105 American Indian alone 35900 49975.0 115550 236550.0 275000 186575.0 6 00105 Asian alone -4800 53275.0 117500 185000.0 672000 131725.0 195 00105 Native Hawaiian and Other Pacific Islander alone 14400 58225.0 72000 134500.0 253200 76275.0 7 00105 Some Other Race alone 18000 50000.0 101900 154750.0 571000 104750.0 44 00105 Two or more races 2900 77425.0 131000 193515.0 635200 116090.0 68 00106 White alone 0 46650.0 90700 156500.0 1047800 109850.0 287 00106 Black or African American alone 0 38600.0 70000 149950.0 1141200 111350.0 47 00106 American Indian alone 55000 89500.0 124000 182015.0 240030 92515.0 3 00106 Alaska Native alone 100000 100000.0 100000 100000.0 100000 0.0 1 00106 American Indian 131500 131500.0 131500 131500.0 131500 0.0 1 00106 Asian alone 0 63875.0 101600 183000.0 729000 119125.0 128 00106 Native Hawaiian and Other Pacific Islander alone 24000 68100.0 105000 129000.0 324720 60900.0 9 00106 Some Other Race alone 0 61850.0 105100 159250.0 592600 97400.0 47 00106 Two or more races 0 51500.0 90700 165200.0 324720 113700.0 47 00107 White alone 0 37200.0 80300 138210.0 704200 101010.0 209 00107 Black or African American alone 990 42900.0 82500 138545.0 570000 95645.0 63 00107 American Indian alone 16800 24100.0 31400 68700.0 106000 44600.0 3 00107 American Indian 18000 28050.0 38400 68475.0 137700 40425.0 4 00107 Asian alone 0 65000.0 100000 170060.0 704200 105060.0 169 00107 Native Hawaiian and Other Pacific Islander alone 14700 67747.5 122855 141452.5 333200 73705.0 14 00107 Some Other Race alone 5000 51600.0 76700 118720.0 594300 67120.0 93 00107 Two or more races 21500 69020.0 103630 166000.0 704200 96980.0 53 00108 White alone 0 77100.0 118900 186600.0 779000 109500.0 169 00108 Black or African American alone 12000 36900.0 109500 231160.0 408750 194260.0 17 00108 American Indian alone 23900 71500.0 115400 164550.0 237700 93050.0 6 00108 American Indian 237700 264100.0 290500 321750.0 353000 57650.0 3 00108 Asian alone 0 90050.0 150000 230000.0 840700 139950.0 282 00108 Native Hawaiian and Other Pacific Islander alone 0 27350.0 92960 136425.0 259000 109075.0 10 00108 Some Other Race alone 1000 81320.0 106005 156050.0 328000 74730.0 64 00108 Two or more races 48600 88300.0 145200 190500.0 326710 102200.0 53 00109 White alone -2300 71200.0 124200 199000.0 1587450 127800.0 265 00109 Black or African American alone 9600 66700.0 99270 185500.0 426500 118800.0 25 00109 American Indian alone 53000 60300.0 86700 135800.0 207900 75500.0 8 00109 Alaska Native alone 138000 138000.0 138000 138000.0 138000 0.0 1 00109 American Indian 51000 217000.0 383000 549000.0 715000 332000.0 2 00109 Asian alone 0 97275.0 172600 263100.0 1587450 165825.0 424 00109 Native Hawaiian and Other Pacific Islander alone 32000 69675.0 136000 440800.0 610000 371125.0 8 00109 Some Other Race alone 12400 63707.5 110395 170000.0 454400 106292.5 46 00109 Two or more races 6400 85727.5 144750 240800.0 1587450 155072.5 56 00110 White alone 0 79400.0 148000 217000.0 785000 137600.0 585 00110 Black or African American alone 4800 50000.0 86000 228000.0 565000 178000.0 21 00110 American Indian alone 1000 105475.0 144205 163582.5 210000 58107.5 4 00110 Alaska Native alone 100500 100500.0 100500 100500.0 100500 0.0 1 00110 American Indian 63800 115850.0 167900 219950.0 272000 104100.0 2 00110 Asian alone -4600 123400.0 180000 258090.0 1130000 134690.0 255 00110 Native Hawaiian and Other Pacific Islander alone 1000 109400.0 136265 247075.0 612000 137675.0 8 00110 Some Other Race alone 26700 73752.5 95950 154775.0 577400 81022.5 28 00110 Two or more races 18400 102500.0 174500 274250.0 715000 171750.0 70 08501 White alone -2300 85212.5 176850 369950.0 2580000 284737.5 634 08501 Black or African American alone 4200 46000.0 99500 139275.0 404090 93275.0 14 08501 American Indian alone 87100 87100.0 87100 87100.0 87100 0.0 1 08501 American Indian 146700 146700.0 146700 146700.0 146700 0.0 1 08501 Asian alone 0 105000.0 218200 385710.0 1836000 280710.0 335 08501 Some Other Race alone 2000 30250.0 106000 216337.5 615400 186087.5 24 08501 Two or more races 13550 132750.0 241200 485075.0 2580000 352325.0 64 08502 White alone 1900 67000.0 137215 241370.0 1008000 174370.0 284 08502 Black or African American alone 11410 33500.0 52400 87750.0 330000 54250.0 10 08502 American Indian alone 54000 58100.0 62200 79450.0 96700 21350.0 3 08502 American Indian 86500 86500.0 86500 86500.0 86500 0.0 1 08502 Asian alone 0 98275.0 174975 285270.0 1008000 186995.0 318 08502 Native Hawaiian and Other Pacific Islander alone 49000 71575.0 94150 116725.0 139300 45150.0 2 08502 Some Other Race alone 8200 57425.0 88050 128850.0 522600 71425.0 20 08502 Two or more races 27700 108450.0 183700 319532.5 841000 211082.5 52 08503 White alone 0 64000.0 120000 198000.0 744900 134000.0 285 08503 Black or African American alone 24000 64400.0 145000 188700.0 231500 124300.0 11 08503 American Indian alone 38500 88975.0 112900 140000.0 200000 51025.0 4 08503 Asian alone 200 100000.0 151120 229015.0 744900 129015.0 231 08503 Native Hawaiian and Other Pacific Islander alone 82000 82000.0 82000 82000.0 82000 0.0 1 08503 Some Other Race alone 35000 72250.0 112000 157625.0 440000 85375.0 19 08503 Two or more races 14200 81025.0 135300 216955.0 329280 135930.0 32 08504 White alone 0 74100.0 136000 225500.0 831600 151400.0 121 08504 Black or African American alone 10000 47000.0 73000 108000.0 205000 61000.0 9 08504 American Indian alone 99300 155650.0 212000 313100.0 414200 157450.0 3 08504 American Indian 22000 41325.0 60650 79975.0 99300 38650.0 2 08504 Asian alone 0 80000.0 143000 230300.0 595000 150300.0 345 08504 Some Other Race alone 0 68075.0 124000 194250.0 292800 126175.0 18 08504 Two or more races 17300 92187.5 167500 305700.0 609000 213512.5 30 08505 White alone 0 43400.0 102900 161005.0 782300 117605.0 143 08505 Black or African American alone 3000 62750.0 101650 181575.0 319000 118825.0 16 08505 American Indian alone 6300 65000.0 85700 156900.0 159160 91900.0 5 08505 American Indian 41200 68825.0 96450 124075.0 151700 55250.0 2 08505 Asian alone 9700 91000.0 150000 221725.0 565000 130725.0 163 08505 Native Hawaiian and Other Pacific Islander alone 60000 122200.0 184400 246600.0 308800 124400.0 2 08505 Some Other Race alone 0 63000.0 103500 143507.5 382000 80507.5 56 08505 Two or more races 30500 91200.0 137600 213410.0 382000 122210.0 17 08506 White alone 10 65225.0 125750 200000.0 990700 134775.0 326 08506 Black or African American alone 13600 81000.0 111600 288000.0 712000 207000.0 12 08506 American Indian alone 22000 61550.0 101100 103300.0 105500 41750.0 3 08506 American Indian 84000 84000.0 84000 84000.0 84000 0.0 1 08506 Asian alone 7800 78825.0 136000 198500.0 990700 119675.0 56 08506 Native Hawaiian and Other Pacific Islander alone 150000 150000.0 150000 150000.0 150000 0.0 1 08506 Some Other Race alone 23000 63000.0 96400 185000.0 415800 122000.0 21 08506 Two or more races 10900 99510.0 142750 185832.5 415800 86322.5 28 08507 White alone 0 82400.0 174050 326000.0 1483000 243600.0 318 08507 Black or African American alone 83000 97500.0 112000 120000.0 128000 22500.0 3 08507 Asian alone 0 120850.0 204500 324300.0 1886000 203450.0 240 08507 Some Other Race alone 219800 463450.0 707100 950750.0 1194400 487300.0 2 08507 Two or more races 30000 130700.0 243155 549527.5 1101000 418827.5 30 08508 White alone 0 52375.0 95750 194650.0 765000 142275.0 266 08508 Black or African American alone 5500 41925.0 71005 114750.0 300000 72825.0 12 08508 American Indian alone 118000 163500.0 209000 254500.0 300000 91000.0 2 08508 Asian alone 3100 104500.0 183500 262500.0 795000 158000.0 175 08508 Some Other Race alone 13200 61750.0 100000 162000.0 233500 100250.0 19 08508 Two or more races 36000 70950.0 150000 204000.0 311600 133050.0 31 08509 White alone 0 54600.0 100210 191000.0 736300 136400.0 297 08509 Black or African American alone 19400 66000.0 100400 167000.0 345000 101000.0 21 08509 American Indian alone 0 16850.0 33700 40700.0 47700 23850.0 3 08509 American Indian 36000 47000.0 58000 69000.0 80000 22000.0 2 08509 Asian alone -2300 49500.0 99320 195250.0 736300 145750.0 120 08509 Native Hawaiian and Other Pacific Islander alone 12000 21000.0 30000 39000.0 48000 18000.0 2 08509 Some Other Race alone 0 50000.0 98500 150400.0 495000 100400.0 69 08509 Two or more races 0 69250.0 114700 244000.0 628300 174750.0 52 08510 White alone 0 61500.0 115500 220000.0 1137000 158500.0 323 08510 Black or African American alone 8440 40650.0 80000 131500.0 671750 90850.0 23 08510 American Indian alone 11400 92850.0 140600 249150.0 513000 156300.0 4 08510 American Indian 245000 327500.0 410000 492500.0 575000 165000.0 2 08510 Asian alone 300 38300.0 99000 185000.0 918000 146700.0 169 08510 Native Hawaiian and Other Pacific Islander alone 60700 64275.0 67850 71425.0 75000 7150.0 2 08510 Some Other Race alone 0 40000.0 65000 105800.0 255000 65800.0 49 08510 Two or more races 11200 71650.0 121000 210100.0 918000 138450.0 47 08511 White alone 0 63725.0 131380 209915.0 750000 146190.0 270 08511 Black or African American alone 25000 50850.0 66200 136250.0 357750 85400.0 11 08511 American Indian alone 47000 47750.0 55000 67775.0 85100 20025.0 4 08511 American Indian 36000 36000.0 36000 36000.0 36000 0.0 1 08511 Asian alone 9010 94215.0 155000 238000.0 835000 143785.0 111 08511 Native Hawaiian and Other Pacific Islander alone 184500 184500.0 184500 184500.0 184500 0.0 1 08511 Some Other Race alone 19800 82600.0 108150 157500.0 315000 74900.0 14 08511 Two or more races 42300 85050.0 141000 315500.0 745100 230450.0 39 08512 White alone 0 78800.0 145750 245417.5 1132400 166617.5 318 08512 Black or African American alone 900 65650.0 124000 173500.0 280000 107850.0 12 08512 American Indian alone 204800 204800.0 204800 204800.0 204800 0.0 1 08512 American Indian 135000 135000.0 135000 135000.0 135000 0.0 1 08512 Asian alone 0 128765.0 190185 295050.0 722900 166285.0 164 08512 Native Hawaiian and Other Pacific Islander alone 248000 248000.0 248000 248000.0 248000 0.0 1 08512 Some Other Race alone 28050 68850.0 109350 164500.0 235000 95650.0 19 08512 Two or more races 9200 116400.0 188750 288900.0 647000 172500.0 42 08514 White alone 0 42200.0 79000 140700.0 603140 98500.0 82 08514 Black or African American alone 1400 19700.0 40000 104500.0 320500 84800.0 11 08514 American Indian alone 55000 58750.0 62500 74700.0 103800 15950.0 4 08514 Asian alone 0 41700.0 107000 167450.0 426000 125750.0 182 08514 Native Hawaiian and Other Pacific Islander alone 72000 84000.0 96000 108000.0 120000 24000.0 2 08514 Some Other Race alone 1400 42040.0 65000 124100.0 320500 82060.0 81 08514 Two or more races 10000 64800.0 121350 201125.0 365300 136325.0 22 "],
["inference.html", "5 Inference 5.1 Sample Distribution 5.2 Normal Distribution 5.3 Confidence Intervals 5.4 OLD VERSION 5.5 Corona Virus 5.6 Sample Distribution", " 5 Inference knitr::opts_chunk$set(warning=FALSE, message=FALSE, fig.width=5, fig.align=&quot;center&quot;) # No warnings library(dplyr) # For pipe and other data commands library(tidyr) # For drop_na library(janitor) # For tabyl library(ggplot2) # For plotting using ggplot() function library(knitr) # For making tablues using kabble() #library(kableExtra) # For styling load(&quot;~/Data/output/ACS_clean.RData&quot;) ls() ## [1] &quot;ACS2018_ASC_raw&quot; &quot;mydata_clean&quot; &quot;mydata_raw&quot; The Census Bureau funds the ACS to provide timely information on a yearly basis. The data is used to make decisions about funding, determine how communities, and much more. This leads to the question–how accurately does a sample represent the population? Our goal is quantify this. When the results of a sample are used to make an educated guess about a population, we call this inference. This implies there is a one-to-one correspondence between what is seen in the sample and the population. As an example, suppose we want to know the percentage of renters in a city. We can infer this value by taking a random sample of households and finding the percentage of renters in the sample. The two should values should be fairly close if the sample is random. The value we are actually want to know (about the population) is called the true value. The information we actually have (from the sample) is called the estimate. This process of using the sample to estimate the true value of a population works but there are kinks to work out. How well and how often do the sample estimates match the true value of the population? Usually never, in the sense that they are exactly the same. Usually it’s pretty close though. The question is how close is it? Is it always close? What’s the worst case scenario? 5.1 Sample Distribution We will investigate how well random samples represent a population. In the real world, you lack information about the population and can only get information about it by trying samples. Luckily we can use computers to simulate how well this process works. We are going to pretend that our sample is the population. mydata_clean %&gt;% distinct(SERIALNO, RNTP) %&gt;% ggplot(aes(RNTP)) + geom_histogram(binwidth = 250, closed = &quot;left&quot;, boundary = 0, color = &quot;white&quot;) The distribution is symmetric so the proper measure of center and spread are mean and standard deviation. mydata_clean %&gt;% distinct(SERIALNO, RNTP) %&gt;% summarize(Mean = mean(RNTP, na.rm = T), SD = sd(RNTP, na.rm = T)) %&gt;% kable() Mean SD 2004.136 892.5093 The question is: If we take samples will the mean of the sample “estimate” be close to the mean of the population “true-value”? How often will the “estimate” be close to the “true-value”? Is there a worst case scenario e.g. the “estimate” will always be no more or no less than \\(x\\) of the “true-value”? 5.1.1 Simulating the sample distribution A sample distribution is made by repeating this process. Take a sample of size \\(n\\) from the population. Calculate a value such as an average of the sample. Store the value somewhere The sample distribution is the collection of values from the different samples. If you calculate a mean from each sample then the sample distribution is the collection of means. A sample distribution has as many values as samples. Usually there will be several thousand or million samples. We will take a thousand samples. population &lt;- mydata_clean %&gt;% distinct(SERIALNO, RNTP) %&gt;% drop_na() population %&gt;% sample_n(30) %&gt;% summarize(Mean = mean(RNTP, na.rm = T), SD = sd(RNTP, na.rm = T)) %&gt;% kable() Mean SD 1937.667 900.5389 This one sample worked fairly well but what we want to do is take lots of samples. We will analyze this sample distribution to quantify how well random sampling works. population &lt;- mydata_clean %&gt;% distinct(SERIALNO, RNTP) %&gt;% drop_na() %&gt;% pull(RNTP) take_sample &lt;- function() { population %&gt;% sample(30) %&gt;% mean() } sample_distribution &lt;- replicate(take_sample(), n=1000) mean(sample_distribution) ## [1] 2004.055 hist(sample_distribution) 5.2 Normal Distribution 5.3 Confidence Intervals 5.4 OLD VERSION We continue with examining the association or independence of two categorical variables. mydata_clean %&gt;% tabyl(new_RAC1P) %&gt;% adorn_totals() %&gt;% kable() new_RAC1P n percent White alone 16031 0.4498288 Black or African American alone 1739 0.0487962 American Indian alone 157 0.0044054 Alaska Native alone 4 0.0001122 American Indian 71 0.0019923 Asian alone 12456 0.3495146 Native Hawaiian and Other Pacific Islander alone 200 0.0056120 Some Other Race alone 2946 0.0826646 Two or more races 2034 0.0570739 Total 35638 1.0000000 sample_distribution &lt;- function(x) { replicate(1000, mydata_clean$new_RAC1P %&gt;% sample(size = x, replace=T) %&gt;% table() %&gt;% getElement(2) %&gt;% (function(i) {i/x})() ) %&gt;% hist(main = paste(&quot;Size of sample:&quot;, x, sep=&quot; &quot;)) } lapply(c(10, 50, 100, 250, 500, 1000, 2000, 3000), sample_distribution) replicate(1, mydata_clean$new_RAC1P %&gt;% sample(size = 20, replace=T) %&gt;% table() ) ## ## . [,1] ## White alone 12 ## Black or African American alone 0 ## American Indian alone 0 ## Alaska Native alone 0 ## American Indian 0 ## Asian alone 8 ## Native Hawaiian and Other Pacific Islander alone 0 ## Some Other Race alone 0 ## Two or more races 0 sample_distribution &lt;- function(x) { mixed &lt;- replicate(1000, mydata_clean$new_RAC1P %&gt;% sample(size = x, replace=T) %&gt;% table() %&gt;% getElement(9) %&gt;% (function(i) {i/x})() ) tibble(mixed) %&gt;% ggplot(aes(x=mixed)) + geom_histogram(aes(y=..density..), boundary = 0, closed = &quot;left&quot;, #binwidth=0.01, binwidth=round(1/x, 3), color=&quot;white&quot; ) + geom_density() + ggtitle(paste(&quot;Size of sample: &quot;, x, &quot;\\nmean: &quot;, round(mean(mixed), 3), &quot;\\nsd: &quot;, round(sd(mixed), 3), sep=&quot;&quot;)) } lapply(c(5, 10, 50, 100, 500), sample_distribution) ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] ## ## [[5]] 5.5 Corona Virus You’re the local health official in a small town in Italy. The future of 10,000 is in your hands. This is the population. You don’t know it but 8% of the citizens have Corona virus. pop %&gt;% tabyl(infection) %&gt;% kable() infection n percent infected 800 0.08 not infected 9200 0.92 More information you don’t know, 18.6% of citizens currently have symptoms but don’t necessarily have the virus. You aren’t like the Chinese. You can’t afford to test everyone. You only have 100 test kits available. How likely are the results in your sample to reflect the actual 8%? pop %&gt;% tabyl(symptoms) %&gt;% kable() symptoms n percent asymptomatic 8137 0.8137 symptomatic 1863 0.1863 You have two options: Test a random sample of 100 people. Test a random sample of 100 people who have symptoms. Which do you think will give you a more accurate answer? Here are the results of a random sample of 100 people. The results are fairly close. pop %&gt;% sample_n(100) %&gt;% tabyl(infection) %&gt;% kable() infection n percent infected 5 0.05 not infected 95 0.95 Here are the results of a random sample of 100 people who had symptoms. The apparent percentage of infections is much too high. pop %&gt;% filter(symptoms == &quot;symptomatic&quot;) %&gt;% sample_n(100) %&gt;% tabyl(infection) %&gt;% kable() infection n percent infected 26 0.26 not infected 74 0.74 pop %&gt;% filter(symptoms == &quot;asymptomatic&quot;) %&gt;% sample_n(100) %&gt;% tabyl(infection) %&gt;% kable() infection n percent infected 3 0.03 not infected 97 0.97 5.6 Sample Distribution Some samples are better than others as we saw. But even a random sample can be horribly wrong. Suppose you kept testing groups of 100 people over and over. Each random sample would have a slightly different rate of infections. This is called variability (each sample is different due to randomness). How different from the actual rate of infection (8%) might a random sample get? This is called simulation of a sample distribution. Here are the results for 1,000 different samples of 100 symptomatic citizens. Some samples had as low as 20% infected but some had as high as 50% infected. The typical value or average value for the different samples is 29% which is way too high. Here are the results for 1,000 different samples of 100 asymptomatic citizens. Some samples had as low as 0% infected but some had as high as 100% infected. The typical value or average value for the different samples is 6.8% which is very close to the real value but the different samples are all over the place. Here are the results for 1,000 different samples of 100 random citizens (with no bias towards lack of or presence of symptoms). Some samples had as low as 4% infected but some had as high as 16% infected. The typical value or average value for the different samples is 8.1% which is correct! This distribution does the best job and in general is fairly close to the correct answer even if sometimes it’s not quite right. "],
["regression.html", "6 Regression 6.1 Simple Linear Regression 6.2 Multiple Regression 6.3 Logistic Regression", " 6 Regression A regression model attempts quantify how explanatory variables contributed or lead to the the response variable. In other words we use the explanatory variable to predict the response variable. The simplest case is two numeric variables. Did the first numeric variable determine, at all, the outcome of the other numeric variable. This is simple linear regression. The next case is for several numeric variables. Did the explanatory variables determine the outcome of the response numeric variable. For instance how well do height, food intake, and blood pressure determine weight? This is multiple linear regression. The next case uses numeric and categorical variables to determine a numeric variable. How well do race and SAT scores predict future income? This is technically called a general linear model but it is often just called multiple linear regression. Last case uses numeric and categorical variables to determine a categorical variables. How do income, ZIP code, and education predict race? This is called logistic regression. The goal is to create models to make predictions but also to assess how well the predictions did. Increasingly science, business, and other decisions use models to see predict what effect a policy change will have. 6.1 Simple Linear Regression 6.2 Multiple Regression 6.3 Logistic Regression "],
["tests-of-independence.html", "7 Tests of Independence", " 7 Tests of Independence We continue with examining the association or independence of two categorical variables. knitr::opts_chunk$set(warning=FALSE, message=FALSE, fig.width=5, fig.align=&quot;center&quot;) # No warnings library(dplyr) # For pipe and other data commands library(janitor) # For tabyl library(ggplot2) # For plotting using ggplot() function library(knitr) # For making tablues using kabble() library(kableExtra) # For styling load(&quot;~/Data/output/ACS_clean.RData&quot;) ls() ## [1] &quot;ACS2018_ASC_raw&quot; &quot;infection&quot; &quot;mydata_clean&quot; ## [4] &quot;mydata_raw&quot; &quot;n&quot; &quot;pop&quot; ## [7] &quot;population&quot; &quot;sample_distribution&quot; &quot;symptoms&quot; ## [10] &quot;take_sample&quot; mydata_clean %&gt;% tabyl(JWTR_new) %&gt;% kable() %&gt;% kable_styling(full_width = F, position=&quot;center&quot;) JWTR_new n percent valid_percent Car, truck, or van 13693 0.3842247 0.7780556 Bus or trolley bus 626 0.0175655 0.0355702 Streetcar 25 0.0007015 0.0014205 Subway 759 0.0212975 0.0431275 Railroad 240 0.0067344 0.0136371 Ferryboat 46 0.0012908 0.0026138 Taxicab 64 0.0017958 0.0036366 Motorcycle 62 0.0017397 0.0035229 Bicycle 305 0.0085583 0.0173305 Walked 527 0.0147876 0.0299449 Worked at home 1082 0.0303609 0.0614808 Other method 170 0.0047702 0.0096596 NA 18039 0.5061732 NA stuff &lt;- mydata_clean %&gt;% distinct(SERIALNO, PINCP, .keep_all = TRUE) %&gt;% filter(!is.na(PINCP), !is.na(JWTR_new), PINCP &gt;= 0, PINCP &lt;= 250000) %&gt;% mutate(JWTR_new = recode(JWTR_new, &quot;Bus or trolley bus&quot; = &quot;Other&quot;, &quot;Streetcar&quot; = &quot;Other&quot;, &quot;Subway&quot; = &quot;Other&quot;, &quot;Taxicab&quot; = &quot;Other&quot;, &quot;Motorcycle&quot; = &quot;Other&quot;, &quot;Ferryboat&quot; = &quot;Other&quot;, &quot;Railroad&quot; = &quot;Other&quot;, &quot;Worked at home&quot; = &quot;Other&quot;, &quot;Other method&quot; = &quot;Other&quot; )) %&gt;% mutate(PINCP_binned = cut(PINCP, breaks=seq(from=0, to=250000, by=50000), dig.lab=7, right = FALSE )) %&gt;% filter(!is.na(PINCP_binned)) stuff %&gt;% tabyl(PINCP_binned, JWTR_new) %&gt;% adorn_percentages(&quot;row&quot;) %&gt;% adorn_pct_formatting(digits = 0) %&gt;% kable() %&gt;% kable_styling(full_width = F, position=&quot;center&quot;) PINCP_binned Car, truck, or van Other Bicycle Walked [0,50000) 77% 17% 2% 5% [50000,100000) 79% 18% 2% 2% [100000,150000) 78% 19% 2% 2% [150000,200000) 78% 19% 1% 2% [200000,250000) 77% 20% 1% 1% tbl &lt;- stuff %&gt;% {table(.$PINCP_binned, .$JWTR_new)} tbl ## ## Car, truck, or van Other Bicycle Walked ## [0,50000) 4965 1075 105 326 ## [50000,100000) 3713 843 81 90 ## [100000,150000) 2092 500 53 47 ## [150000,200000) 1159 287 20 23 ## [200000,250000) 574 150 11 8 chisq.test(tbl) ## ## Pearson&#39;s Chi-squared test ## ## data: tbl ## X-squared = 153.2, df = 12, p-value &lt; 2.2e-16 "]
]
